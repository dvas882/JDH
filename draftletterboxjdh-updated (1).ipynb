{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# Local and global geographies. Shell companies in Luxembourg (1929-2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "contributor"
    ]
   },
   "source": [
    " ### Benoît  Majerus [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0003-4869-2061) \n",
    "Centre for Contemporary and Digital History, University of Luxembourg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Contributor2FirstName  Contributor2LastName [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/ORCID_ID_IF_EXIST) \n",
    "Institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Demival  Vasques Filho [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-4552-0427) \n",
    "Centre for Contemporary and Digital History, University of Luxembourg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by-nc-nd](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "cover"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"./media/placeholder.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "disclaimer"
    ]
   },
   "source": [
    " (optional) This article was orginally published (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "abstract"
    ]
   },
   "source": [
    "This is an abstract (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[we can make comments like this ]: #  \n",
    "\n",
    "[do we need a citation for \"Unshell\"? ]: #  \n",
    "\n",
    "[Narrative layer] In December 2021, the European Commission presented the “Unshell” directive, designed to combat the misuse of shell entities for tax avoidance within the European Union (“Unshell”). This measure was introduced in the context of significant strain on public finances during the COVID-19 pandemic, which saw a substantial increase in public deficits across most European states. However, this directive is part of a broader, long-term effort in global tax governance to curb various forms of tax evasion. A transnational governance of taxation has been in the making since the interwar period, at the latest (Farquet 2010), but moments of intense development were followed by long periods of inactivity. The Unshell directive is part of a longer sequence that started after the financial crisis that hit the world starting in 2007, on the one hand, and by the scandalization of certain practices by journalist consortia, such as the Panama Papers, LuxLeaks, or Cyprus Confidential, on the other hand. The OECD and G20 committed to create an international framework to combat tax avoidance by multinational enterprises, starting from 2013 through the Base Erosion and Profit Shifting (BEPS) Action Plan. Since 2016, the European Union has launched several Anti-Tax Avoidance Packages (ATAD). The Unshell directive is part of a third component of ATAD (Sinnig and Zetzsche 2023). At the time of publishing this article, the Unshell directive is still navigating through the labyrinths of the infamous EU trilogue. Subsequently, the transposition of the directive into each member country will reveal its true implications for the tax chains, with Luxembourg sometimes taking a long time to transpose directives that seem unfavorable to its financial center (Bourbaki 2016). Regardless of the fate of the Unshell directive, it has brought to the fore a tool for tax planning and beneficial ownership avoidance: shell companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] Shell companies are difficult to define: most authors agree that a common trait is the absence of any economic substance and that they are an essential part of several tax havens (Beckett 2023). Be it Switzerland, Luxembourg or the British Virgin Islands… all of them introduced legal structuring of companies that permitted tax reduction and anonymisation, the first two through the holding regime that became a “refuge for capital flight” (Paquier 2001, 179) in the interwar period or through the International Business Company Act in the 1990s for the BVI (Maurer 2000). But till now, they have not attracted much attention till now, one exception being the recent (Weitzman 2022). Shell companies are nonetheless an essential infrastructure in global tax chains: most of the offshore centers offer this legal construct and so does Luxembourg. Although the Grand Duchy of Luxembourg was not among the first generation of tax havens (Guex 2022; Watteyne 2023), by the late 1920s it had adopted a legal structure—the holding company—that positioned it in the market for tax engineering and the anonymization of actual beneficiaries (Calabrese and Majerus 2023).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] Based on the publicly available company registers, this article maps the shell companies as an essential infrastructure of the Luxembourg financial offshore center by analyzing on the one hand how they were inscribed in the local geography of the capital of the Grand-Duchy, and how they linked to other offshore center in larger global tax chains. Geographers have been studying the geography of the Luxembourg financial center for some time now, focusing on its local dynamics as well as its role in global financial geography (Walther, Schulz, and Dörry 2011; Dörry 2016; Hesse and Wong 2019), although they have not greatly emphasized the historical dimension of this process. Historians have so far scarcely explored this topic, focusing mainly on banks (Pauly 2018; Duval, Gabellini, and Mouton 2023). While banks are certainly significant actors, they are not the only financial service providers. The establishment of shell companies, whose headquarters are often located in the offices of service providers, allows for a parallel or complementary geography of the financial sector. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw material\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In the midst of World War I, Luxembourg reformed its corporate regulation with the law of August 10, 1915. (‘Loi Du 10 Août 1915 Concernant Les Sociétés Commerciales’ 1915). This law required every company to make public certain information for each company established in Luxembourg. Published as an appendix to the Memorial, the official journal of the Grand Duchy of Luxembourg, these texts were entitled, from 1960 onwards, to a separate publication, Mémorial C (‘Règlement Grand-Ducal Du 9 Janvier 1961 Relatif Aux Trois Recueils Du Mémorial’, n.d.). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hermeneutics layer] Unfortunately the corpus does not exist under a digitised, coherent and semi-structured format. The years 1929-1960 were scanned at the Luxembourg Centre for Contemporary and Digital History (C2DH) on a Treventus ScanRobot 2.0 MDS book scanner (400 dpi in tif format). Thus, Optical Character Recognition (OCR) plays a critical role in extracting text from scanned documents. However, during our work, we encountered several challenges that significantly impacted the accuracy and reliability of the extracted text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hermeneutics layer] One of the primary issues was related to the layout of the scanned documents. The pages contained white margins that acted as long 2D separators, inadvertently segmenting business descriptions. This disrupted the logical flow of the text and led to incorrect parsing. Additionally, some pages were distorted and skewed, tilting either left or right, which further complicated text extraction and alignment (Fig XXX). These challenges underscore the complexities associated with OCR processing when dealing with inconsistent document layouts, distortions. To deal with this problem programmatically, it was necessary to implement advanced preprocessing techniques such as margin removal, deskewing algorithms, and language-specific OCR models. These enhancements would help mitigate errors and ensure more reliable text extraction in the OCR application. However, since the number of mentioned businesses in the documents for this period was not considerable, and the above-mentioned methods require significant time and effort to implement, we decided to process this time period manually. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hermeneutics layer] Moving forward, the years 1961-1995 were digitised by the National Library of Luxembourg (BNL). Thanks to a convention with the BNL, our research project got access to these files that are publically available on e-luxemburgensia but with major restrictions.  For the last remaining decades (1996-2016), we downloaded the pdf files from legilux.public.lu. Although these scanned documents were of relatively high quality, challenges remained in extracting text using OCR. Notably, the multilingual nature of the documents—written in French, German, and Luxembourgish—introduced additional complications. Accented characters such as é, ä, ü, and ö were frequently misrecognized, leading to character-level errors. Punctuation marks also caused ambiguities: for example, a period following a capital letter (e.g., \"I.\") was sometimes misread as \"L\", while \"O.\" was incorrectly identified as \"Q\". These challenges affected the quality of information extracted from the scanned documents. To improve accuracy, we employed generative AI (LLMs), combined with human review, as part of a post-cleaning and information enhancement process to mitigate the impact of these issues on the extracted business descriptions. It’s worth noting that, despite these refinements, the dataset contains approximately 3 million business descriptions from 1961 to 2016. Assuming an average of 30 seconds per post-cleaning task, it would take roughly three years to process the entire dataset. Therefore, some degree of inaccuracy remains and is expected to be gradually corrected in the coming years..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[footnotes]\n",
    "2.   “The digitized version of the Memorial C (1915-1940, 1960-1995), which also contains personal data, is not subject to indexing and search, except for the titles of the articles, in compliance with the GDPR. The PDF files of the various issues, articles, and pages cannot be downloaded by users.” translated by the authors https://eluxemburgensia.lu/periodicals/memorialc (last accessed: 16 February 2024)\n",
    "3.  Unfortunately the Luxembourgish administration refused to give us access to the structured data they possess for the years 1996-2016: we had only access to the ‘unstructured’ pdfs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In summary, the register of companies of Luxembourg is publicly available: for the years 1940 to 1959 on paper in some Luxembourgish libraries, for the years 1929 to 1939 and 1961 to 2016 on the web. The policies of publicity are however different: paradoxically the BNL does protect the older data (1929-1939, 1961-1996) more strictly than the Prime Minister administration, responsible for the website legilux.public.lu that gives you access to the years 1996-2016.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] The register of companies does not fall under the restriction of the Luxembourgish Archival Law as it is publicly available from the day it is published. But as data is collected, analyzed, stored and processed, the General Data Protection Regulation’s (GDPR) that has been implemented in the European Union since 2018, applies. If the GDPR explicitly provides for exceptions for research, the wording remains rather vague. While there is today a growing body of case law concerning certain data collections and publications - notably for services such as online tracking (offered for example Google) or regarding the work of journalists (Bitiukova 2023), historians have relatively little formalized their practices, apart from the obligations they may face from those who hold the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] Some of the solutions proposed in the literature are inapplicable in this project. Obtaining authorization of the mentioned names (around 350 000 persons in our database) would require a disproportionate effort. Pseudonymization or anonymization during data collection but also during publication is contrary to the goal of the research, which specifically aims to identify the manufacturing chains of shell companies and to historicize the actors and uncover the societal networks within which they operated (Luyten 2022; Friedewald forthcoming). We will therefore, if necessary for the historical argument, publish the name of financial service providers such as notaries, lawyers or bankers. If the \"Indeterminate legal concepts\" and the “large scopes of interpretation” (Jahnel 2023, 561–62) of the GDPR allows such a practice we inserted nevertheless on the webpage of the project a “your section” allowing people to access and correct information we have collected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data forging: the ETL Workflow for historical records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In this study we processed unstructured data in PDF format (scanned documents) through an ETL (Extract, Transform, Load) workflow. The process involved extracting data using 2D line object detection, transforming it into a structured text format with the help of Large Language Models (LLMs), and finally storing it in a database for further analysis. The entire workflow is depicted in Fig XXX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************\n",
    "                      Figure\n",
    "**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure] Fig XXX: Tilted scanned pages with white margins  (can we add a figure here exemplifying \"pages contained white margins that acted as long 2D separators, inadvertently segmenting business descriptions\"\n",
    ", can we put side by side an example of a document digitised by the BNL and of a PDF downloaded from legilux.public.lu?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In summary, the register of companies of Luxembourg is publicly available: for the years 1940 to 1959 on paper in some Luxembourgish libraries, for the years 1929 to 1939 and 1961 to 2016 on the web. The policies of publicity are however different: paradoxically the BNL does protect the older data (1929-1939, 1961-1996) more strictly than the Prime Minister administration, responsible for the website legilux.public.lu that gives you access to the years 1996-2016.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] The register of companies does not fall under the restriction of the Luxembourgish Archival Law as it is publicly available from the day it is published. But as data is collected, analyzed, stored and processed, the General Data Protection Regulation’s (GDPR) that has been implemented in the European Union since 2018, applies. If the GDPR explicitly provides for exceptions for research, the wording remains rather vague. While there is today a growing body of case law concerning certain data collections and publications - notably for services such as online tracking (offered for example Google) or regarding the work of journalists (Bitiukova 2023), historians have relatively little formalized their practices, apart from the obligations they may face from those who hold the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] Some of the solutions proposed in the literature are inapplicable in this project. Obtaining authorization of the mentioned names (around 350 000 persons in our database) would require a disproportionate effort. Pseudonymization or anonymization during data collection but also during publication is contrary to the goal of the research, which specifically aims to identify the manufacturing chains of shell companies and to historicize the actors and uncover the societal networks within which they operated (Luyten 2022; Friedewald forthcoming). We will therefore, if necessary for the historical argument, publish the name of financial service providers such as notaries, lawyers or bankers. If the \"Indeterminate legal concepts\" and the “large scopes of interpretation” (Jahnel 2023, 561–62) of the GDPR allows such a practice we inserted nevertheless on the webpage of the project a “your section” allowing people to access and correct information we have collected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "This is a hermeneutic paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jdh": {
     "module": "object",
     "object": {
      "source": [
       "table 1: label table 1"
      ]
     }
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "table-1"
    ]
   },
   "source": [
    "Editor|1641|1798|1916\n",
    "---|---|---|---\n",
    "Senan|0.55|0.4|0.3\n",
    "Henry|0.71|0.5|0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data forging: the ETL Workflow for historical records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In this study we processed unstructured data in PDF format (scanned documents) through an ETL (Extract, Transform, Load) workflow. The process involved extracting data using 2D line object detection, transforming it into a structured text format with the help of Large Language Models (LLMs), and finally storing it in a database for further analysis. The entire workflow is depicted in Fig XXXYYY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************\n",
    "                      Figure\n",
    "**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure] Fig XXX:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In the Extract step of the ETL workflow, we process unstructured PDF documents to extract text data using a combination of OCR (Optical Character Recognition) and advanced image preprocessing techniques. First, each page of the PDF is converted into an image using the pdfplumber library. These images are preprocessed by converting them to grayscale and applying binary thresholding to enhance text visibility. Line detection is then performed using morphological operations to identify horizontal structures, segmenting the image into manageable regions. The extracted regions are processed using Tesseract OCR with multilingual support (configured for languages such as French, German, Luxembourgish, and English) to obtain high-quality text data. The extracted text is refined and organized into blocks based on line and paragraph alignment, ensuring readability and accuracy. This extracted text is then aggregated, structured, and saved to a text file for subsequent transformation and loading steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work efficiently with image and PDF data in Python, it is essential to install a set of libraries that support text recognition, document parsing, and model training. pytesseract is a Python wrapper for Google’s Tesseract-OCR Engine, enabling accurate text extraction from images using Optical Character Recognition (OCR). opencv-python (OpenCV) provides powerful tools for image processing, including reading, displaying, and transforming images or video frames. pdfplumber allows structured extraction of text, tables, and metadata from PDF documents, making it especially useful for digitizing and analyzing scanned content. For machine learning and model fine-tuning tasks, additional libraries are required. peft (Parameter-Efficient Fine-Tuning) and trl (Transformers Reinforcement Learning) support advanced training techniques, such as LoRA-based fine-tuning and supervised fine-tuning workflows. tensorboard and tensorboardX provide visualization tools to monitor training performance and metrics in real time. Together, these libraries create a robust environment for building end-to-end pipelines that can extract, process, and analyze text from both visual documents and large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "!pip install pytesseract\n",
    "!pip install opencv-python\n",
    "!pip install pdfplumber\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install tensorboard\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Extract step of the ETL workflow, unstructured PDF documents are processed to extract text data using a combination of Optical Character Recognition (OCR) and advanced image preprocessing techniques. Each page of the PDF is first converted into an image using the pdfplumber library. These images are then preprocessed by converting them to grayscale and applying binary thresholding to enhance text visibility. Line detection uses morphological operations to identify horizontal structures, which segment the image into manageable regions. The segmented areas are processed using Tesseract OCR, configured with multilingual support for languages such as French, German, Luxembourgish, and English, to obtain high-quality textual data. The extracted content is subsequently refined and organized into coherent blocks based on line and paragraph alignment, ensuring improved readability and accuracy. Finally, the structured text is aggregated and saved to a text file, preparing it for the subsequent transformation and loading phases. An explanation of the parameters and components used in the extraction process is outlined below. The script begins by importing essential libraries for file handling (os), image processing (cv2, PIL), Optical Character Recognition (pytesseract), PDF parsing (pdfplumber), and data manipulation (pandas, numpy). The Tesseract OCR engine is configured by specifying its executable path, and multilingual support is verified via subprocess to list installed languages.\n",
    "\n",
    "Input and output paths are defined to manage the location of PDF files and the resulting extracted text. The core extract_text function performs preprocessing steps such as grayscale conversion and binary thresholding to improve OCR accuracy. Tesseract is then used to extract text in multiple languages (French, German, Luxembourgish, and English), organizing content into structured text blocks with attention to alignment and spacing.\n",
    "\n",
    "The main processing loop iterates through each PDF in the input directory, converting pages into images and applying morphological operations to detect horizontal lines. These lines are used to segment the page into distinct regions, from which text is extracted and formatted. The refined text from all pages is aggregated and saved as a single structured text file. For validation purposes, matplotlib is used to visualize both the original pages and cropped sections during processing. The final output includes the structured text and a log of the processing status for each file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # For displaying images\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# Configure Tesseract executable path for Windows\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# List installed languages to verify Tesseract setup\n",
    "result = subprocess.run([pytesseract.pytesseract.tesseract_cmd, '--list-langs'], stdout=subprocess.PIPE, text=True)\n",
    "print(\"Installed Tesseract Languages:\\n\", result.stdout)\n",
    "\n",
    "# Path where processed files will be saved\n",
    "pathW = \"\\\\JDH_PAPER\\\\\"  # Adjust path to your environment\n",
    "\n",
    "def extract_text(image):\n",
    "    \"\"\"\n",
    "    Extract text from an image using Tesseract OCR.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image from which text will be extracted.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    except Exception as e:\n",
    "        gray = image\n",
    "        print(f\"Error during grayscale conversion: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Apply binary thresholding to enhance text contrast\n",
    "        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during thresholding: {e}\")\n",
    "        thresh = image\n",
    "\n",
    "    # Configure Tesseract for multilingual OCR\n",
    "    custom_config = r'-c preserve_interword_spaces=1 --oem 1 --psm 6 -l fra+deu+ltz+eng'\n",
    "    \n",
    "    # Extract OCR data from image\n",
    "    d = pytesseract.image_to_data(thresh, config=custom_config, output_type=Output.DICT)\n",
    "    df = pd.DataFrame(d)\n",
    "    df1 = df[(df.conf != '-1') & (df.text != ' ') & (df.text != '')]\n",
    "\n",
    "    # Sort text blocks vertically\n",
    "    sorted_blocks = df1.groupby('block_num').first().sort_values('top').index.tolist()\n",
    "    text = ''\n",
    "    for block in sorted_blocks:\n",
    "        curr = df1[df1['block_num'] == block]\n",
    "        sel = curr[curr.text.str.len() > 5]\n",
    "        char_w = (sel.width / sel.text.str.len()).mean()\n",
    "        prev_par, prev_line, prev_left = 0, 0, 0\n",
    "        for ix, ln in curr.iterrows():\n",
    "            # Add new line when switching paragraphs or lines\n",
    "            if prev_par != ln['par_num']:\n",
    "                text += '\\n'\n",
    "                prev_par = ln['par_num']\n",
    "                prev_line = ln['line_num']\n",
    "                prev_left = 0\n",
    "            elif prev_line != ln['line_num']:\n",
    "                text += '\\n'\n",
    "                prev_line = ln['line_num']\n",
    "                prev_left = 0\n",
    "\n",
    "            # Calculate space adjustments for alignment\n",
    "            added = 0\n",
    "            if ln['left'] / char_w > prev_left + 1:\n",
    "                added = int((ln['left']) / char_w) - prev_left\n",
    "                text += ' ' * added\n",
    "            text += ln['text'] + ' '\n",
    "            prev_left += len(ln['text']) + added + 1\n",
    "        text += \" \\n\"\n",
    "    return text\n",
    "\n",
    "# Process each year within a specific range\n",
    "for year in range(1961, 1962):\n",
    "    path = \"\\\\JDH_PAPER\\\\\"  # Adjust this path accordingly\n",
    "    List = os.listdir(path)\n",
    "    previous_end_crop = \"\"  # Stores the last crop of a page to combine with the next page if necessary\n",
    "    \n",
    "    for each in List:\n",
    "        if each.endswith(\".pdf\"):\n",
    "            print(f\"Processing file: {each}\")\n",
    "            pdf_file = os.path.join(path, each)\n",
    "            outputFilesPath = os.path.join(pathW, each.replace(\".pdf\", \".txt\"))\n",
    "            my_pdf = pdfplumber.open(pdf_file)\n",
    "            \n",
    "            y_all = {i: [] for i in range(len(my_pdf.pages))}\n",
    "            All_Text = \"\"\n",
    "            All_Text += \"========================================================== \\n\"\n",
    "            All_Text += f\"\\n +++++++++++++++++ \\n File name: {each} Page Number: {1}\\n +++++++++++++++++ \\n\"\n",
    "            \n",
    "            for i in range(len(my_pdf.pages)):\n",
    "                # Convert each page to an image\n",
    "                im = my_pdf.pages[i].to_image(resolution=420)\n",
    "                print(f\"Processing page {i + 1}\")\n",
    "                im.save(\"\\\\JDH_PAPER\\\\temporary.png\", \"PNG\")\n",
    "                image = cv2.imread(\"\\\\JDH_PAPER\\\\temporary.png\")\n",
    "\n",
    "                # Display the original page\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "                plt.title(f\"Original Page {i}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "                # Preprocess for line detection\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 1))\n",
    "                dilated = cv2.dilate(thresh, kernel, iterations=2)\n",
    "                horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (320, 1))\n",
    "                detected_lines = cv2.morphologyEx(dilated, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
    "\n",
    "                # Detect lines and extract their coordinates\n",
    "                cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "                for c in cnts:\n",
    "                    y = c[0][0][1]\n",
    "                    if y > 0:\n",
    "                        y_all[i].append(y)\n",
    "\n",
    "                y_all[i] = sorted(list(set(y_all[i])))\n",
    "                print(\"Detected line y-coordinates:\", y_all[i])\n",
    "\n",
    "                start = [0] + y_all[i] + [image.shape[0]]\n",
    "\n",
    "                # Crop and process each region between detected lines\n",
    "                for k in range(len(start) - 1):\n",
    "                    cropImage = image[start[k]:start[k + 1], :]\n",
    "                    plt.figure(figsize=(12, 10))\n",
    "                    plt.imshow(cv2.cvtColor(cropImage, cv2.COLOR_BGR2RGB))\n",
    "                    plt.title(f\"Page {i} - Crop {k + 1}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    text = extract_text(cropImage)\n",
    "                    print(\"Extracted text:\", text)\n",
    "\n",
    "                    if k == len(start) - 2 and i != len(my_pdf.pages) - 1:\n",
    "                        previous_end_crop = text\n",
    "                    else:\n",
    "                        All_Text += text\n",
    "                        All_Text += \"\\n ========================================================== \\n\"\n",
    "                        All_Text += f\"\\n +++++++++++++++++ \\n File name: {each} Page Number: {i + 1}\\n +++++++++++++++++ \\n\"\n",
    "                \n",
    "                if previous_end_crop:\n",
    "                    All_Text += previous_end_crop\n",
    "                    previous_end_crop = \"\"\n",
    "            \n",
    "            # Save extracted text to file\n",
    "            with open(outputFilesPath, \"w\") as outputFiles:\n",
    "                outputFiles.write(All_Text)\n",
    "            print(\"Processing completed for:\", each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narrative layer] In the transform phase of the ETL workflow, raw extracted data is cleaned, structured, and enriched to ensure it is in a usable format for analysis. For this paper, we utilized generative AI models, specifically Large Language Models (LLMs), to transform unstructured text into well-structured and semantically enriched formats, enabling more accurate and meaningful insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "Now we explain a code that outlines a workflow to process a text file, extract valid company names using regex cleaning and a generative AI model, and save the results to an output file. It begins by importing essential libraries: transformers for using pre-trained models like Mistral, torch for GPU and model operations, re for pattern matching, and gc for memory cleanup. Two text-cleaning functions handle noise removal: clean_start_of_text_number1 removes leading characters and numbers, while clean_start_of_text_number2 removes text before two-digit numbers, accounting for short and uppercase formats. The Mistral model (microsoft/Orca-2-7b) and tokenizer are initialized with support for GPU and Hugging Face tokens.\n",
    "The validate_company_name function extracts and validates concise company names from descriptions using a structured prompt. extract_company_names generates names based on this prompt, cleans the output, and filters irrelevant cases, such as lines with \"Siège social\" or \"Sitz.\"\n",
    "The main workflow reads the input file line by line, cleans the text, prepares prompts, extracts and validates names, and filters invalid results (e.g., returning \"Wrong\" for non-company content).Memory is optimized during GPU-intensive processing using torch.cuda.empty_cache and gc.collect.\n",
    "Finally, results are saved to extracted_company_names.txt, pairing extracted company names with their original input lines using a delimiter (&&&****&&&) for easy parsing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch,gc\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_start_of_text_number1(input_text):\n",
    "    # Remove any punctuation from the first characters\n",
    "    cleaned_text = re.sub(r'^[^a-zA-ZÀ-ÖØ-öø-ÿÄäÖöÜüßÇçÉéÈèÊêËëÀàÂâÎîÏïÔôÛûÙùŸÿÆæŒœ]*', '', input_text)\n",
    "\n",
    "    # Check if there's a number at the start of the text and remove everything before and including it\n",
    "    match = re.search(r'^\\s*\\d+', cleaned_text)\n",
    "    if match:\n",
    "        # Remove the leading number and any preceding characters\n",
    "        cleaned_text = cleaned_text[match.end():].strip()\n",
    "    else:\n",
    "        # If no number at the start, re-clean remaining leading junk characters\n",
    "        cleaned_text = re.sub(r'^[^\\wÀ-ÖØ-öø-ÿÄäÖöÜüßÇçÉéÈèÊêËëÀàÂâÎîÏïÔôÛûÙùŸÿÆæŒœ]+', '', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_start_of_text_number2(input_text):\n",
    "    # Find the first occurrence of a number with 2 or more digits and remove all text preceding it\n",
    "    if len(input_text)<50:\n",
    "        end1=len(input_text)\n",
    "    else:\n",
    "        end1 =35\n",
    "        if input_text[:end1].isupper()==True:\n",
    "            end1=0\n",
    "    match = re.search(r'\\b\\d{2,}', input_text[:end1])\n",
    "    if match:\n",
    "        # Remove everything before and including the first occurrence of the number\n",
    "        cleaned_text = input_text[match.end():].strip()\n",
    "    else:\n",
    "        # If no number with 2 or more digits is found, return the original input\n",
    "        cleaned_text = input_text.strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "    \n",
    "# Initialize the Mistral model and tokenizer\n",
    "model_name = \"microsoft/Orca-2-7b\"#\"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token='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')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token='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').to(device)\n",
    "\n",
    "def validate_company_name(company_description, suggested_company_name, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Validates and extracts the correct company name based on the provided description and suggested name.\n",
    "\n",
    "    Args:\n",
    "        company_description (str): The company description containing possible company names.\n",
    "        suggested_company_name (str): The suggested full company name.\n",
    "        model (AutoModelForCausalLM): Pretrained language model for causal generation.\n",
    "        tokenizer (AutoTokenizer): Tokenizer corresponding to the pretrained model.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted concise company name.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are tasked with validating and extracting the company name from a description. \"\n",
    "        \"The company name may be written in full or as an abbreviation.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Identify the most concise version of the company name within the provided description.\\n\"\n",
    "        \"2. If an abbreviation or alternate form of the name is explicitly stated, extract that form.\\n\"\n",
    "        \"3. Return only the exact company name as it appears in the text, without any additional explanation or formatting.\\n\\n\"\n",
    "        \"Example 1:\\n\"\n",
    "        \"Company Description: \\\"e OMNIUM INTERNATIONAL S.A.» en abréviation: « OMINTER ». Siège social: Luxembourg, 19, Boulevard Prince Henri.\\\"\\n\"\n",
    "        \"Suggested Company Name: \\\"OMNIUM INTERNATIONAL S.A.\\\"\\n\"\n",
    "        \"Output: \\\"OMINTER\\\"\\n\\n\"\n",
    "        \"Example 2:\\n\"\n",
    "        \"Company Description: \\\"GLOBALTECH INNOVATIONS GmbH, also referred to as 'GLOBTECH'. Headquartered in Berlin.\\\"\\n\"\n",
    "        \"Suggested Company Name: \\\"GLOBALTECH INNOVATIONS GmbH\\\"\\n\"\n",
    "        \"Output: \\\"GLOBTECH\\\"\\n\\n\"\n",
    "        \"Example 3:\\n\"\n",
    "        \"Company Description: \\\"Alpha Corp. Official Name: 'Alpha Corporation'.\\\"\\n\"\n",
    "        \"Suggested Company Name: \\\"Alpha Corp.\\\"\\n\"\n",
    "        \"Output: \\\"Alpha Corporation\\\"\\n\\n\"\n",
    "        \"Company Description: ABC Société Anonyme. fabrique de cuivre, \"\n",
    "        \"Suggested Company Name: \\\"ABC, Société Anonyme.  fabrique de cuivre,\\\"\\n\"\n",
    "        \"Output: \\\"ABC\\\"\\n\\n\"\n",
    "        \"Now process the following:\\n\\n\"\n",
    "        f\"Company Description: {company_description}\\n\"\n",
    "        f\"Suggested Company Name: {suggested_company_name}\\n\"\n",
    "        f\"Output:\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=50, temperature=0.2, do_sample=False)\n",
    "    \n",
    "    # Decode the response and extract the result\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    extracted_name = generated_text.split(\"Output:\")[-1].strip()\n",
    "    return extracted_name\n",
    "\n",
    "# Function to generate company name predictions\n",
    "def extract_company_names(prompt, model, tokenizer):\n",
    "    # Ensure inputs are on the same device as the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=30, eos_token_id=tokenizer.eos_token_id)\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"\\n\",\" \").replace(prompt.replace(\"\\n\",\" \"),\"\").replace(\"\\n\",\"\")\n",
    "    if \"company name:\" in output_text:\n",
    "        output_text=output_text.split(\"company name:\")[1]\n",
    "    if \"Company name:\" in output_text:\n",
    "        output_text=output_text.split(\"Company name:\")[1]\n",
    "    if \"company names:\" in output_text:\n",
    "        output_text=output_text.split(\"company names:\")[1]\n",
    "    if \"Company names:\" in output_text:\n",
    "        output_text=output_text.split(\"Company names:\")[1]\n",
    "    if \"Siège social\" in output_text:\n",
    "        output_text=output_text.split(\"Siège social\")[0]\n",
    "    if \"Siege social\" in output_text:\n",
    "        output_text=output_text.split(\"Siege social\")[0]\n",
    "    if \"Siége social\" in output_text:\n",
    "        output_text=output_text.split(\"Siége social\")[0]\n",
    "    if \"Hauptsitz\" in output_text:\n",
    "        output_text=output_text.split(\"Hauptsitz\")[0]\n",
    "    if \"Sitz\" in output_text:\n",
    "        output_text=output_text.split(\"Sitz\")[0]\n",
    "    if \"sitz\" in output_text:\n",
    "        output_text=output_text.split(\"sitz\")[0]\n",
    "    if \"S. à r.\" in output_text:\n",
    "        output_text=output_text.split(\"S. à r.\")[0]\n",
    "    output_text = clean_start_of_text_number2(output_text)\n",
    "    print(\"Output Text: \", output_text)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# Read the input file line by line\n",
    "input_file = \"/JDH_PAPER/1970-01-23_01_Single_Line.txt\"  # Replace with your input text file\n",
    "output_file = \"/JDH_PAPER/extracted_company_names.txt\"\n",
    "\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        line=line.strip().lstrip().rstrip()\n",
    "        if len(line)>2:\n",
    "            input_text=line.split('+++++++++++++++++')[-1].strip()\n",
    "            if len(input_text)<130:\n",
    "                end=len(input_text)\n",
    "            else:\n",
    "                end=130\n",
    "            if \"SOMMAIRE\" in input_text[:20] or 'MEMORIAL Journal Officiel me Amtsblatt' in input_text[:end] or 'RECUEIL SPECIAL ue DES SOCIETES ET ASSOCIATIONS' in input_text[:end]:\n",
    "                company_names=\"Wrong\"\n",
    "            else:\n",
    "                input_text=input_text[:end]\n",
    "                input_text=clean_start_of_text_number1(input_text)\n",
    "                # input_text=clean_start_of_text(input_text)\n",
    "    \n",
    "                # Prepare the prompt for Mistral with clear instruction\n",
    "                prompt = (\n",
    "                    f\"In the following text, company names are short and exact phrases from the input that appear at the start of the given text and are often found before terms like 'social:', 'RECTIFICATIF', 'Hauptsitz:' or 'Sitz:'. \"\n",
    "                    f\"If the text is too short or it is not part of a company description, answer with 'Wrong'. \"\n",
    "                    f\"It is possible for the given input text to begin with irrelevant characters. For example, in 'TT aaa ttt eeee aa oo 45 ALTA S. A. H. Siège social: Luxembourg,' the company name is 'ALTA S. A. H.' \"\n",
    "                    f\"Extract the exact company names from the provided text without any additional explanation. The company name must be an exact substring of the input text. You must only provide the exact company name:\\n\\n\"\n",
    "                    f\"{input_text.strip()}\\n\\n\"\n",
    "                    f\"Company Names:\"\n",
    "                )\n",
    "    \n",
    "    \n",
    "                print(\"prompt:\",prompt)\n",
    "                print(\"==========\")\n",
    "                generated_text = extract_company_names(prompt, model, tokenizer)\n",
    "                generated_text= generated_text.rstrip().lstrip().strip()\n",
    "                company_names = generated_text\n",
    "                del generated_text\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                result = validate_company_name(input_text.strip(), company_names, model, tokenizer)\n",
    "                company_names = result\n",
    "                print(\"Final company_names:\",company_names)\n",
    "                print(\"***********************************\")\n",
    "\n",
    "                del result\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                # Filter valid company names\n",
    "            \n",
    "            # Write each company name to the output file\n",
    "            outfile.write(company_names +\" &&&****&&& \"+line.replace(\"\\n\",\"\")+ \"\\n\")\n",
    "\n",
    "print(f\"Extraction completed. Company names are saved in extracted_company_names.txt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we focus on another part of the implementation: extracting and validating individuals’ names from text using a combination of regular expressions and a generative AI model. This component is designed to handle multilingual and unstructured inputs effectively. The code begins by importing essential libraries—regex for advanced pattern matching with Unicode support, and transformers and torch to load the Mistral model (mistralai/Mistral-7B-Instruct-v0.3) and configure it for execution on GPU or CPU, with secure access via a Hugging Face authentication token.\n",
    "\n",
    "The core logic includes a function, extract_sequences, which identifies personal name patterns such as initials, full names with titles, capitalized sequences, and names with linguistic prefixes. These matches are consolidated into a filtered list of potential names. Additional functions like clean and clean_name ensure consistent formatting by removing excess whitespace, handling uppercase sequences, and accounting for cultural name structures.\n",
    "\n",
    "To validate whether a string is likely a personal name, the is_person_name function constructs a structured prompt for the Mistral model and generates low-randomness responses for consistency. The file processing workflow reads input data line by line, splits each line using the delimiter &&&****&&&, and extracts the segment to be processed. Regex is first applied to extract name candidates, which are then validated by the AI model and formatted accordingly.\n",
    "\n",
    "The final output includes the company name, validated personal names, and the original raw input, written to a new file with _people.txt appended to the filename. This approach enables scalable, efficient, and language-agnostic name extraction while ensuring robustness against variations in formatting, abbreviations, and irrelevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re  # Importing regex module which supports Unicode property escapes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# Replace 'your_huggingface_token' with your actual Hugging Face access token\n",
    "token = 'your_token'\n",
    "\n",
    "# Load the model and tokenizer with the token for authentication\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token).to(device)\n",
    "\n",
    "def extract_sequences(text):\n",
    "    \"\"\"\n",
    "    Extract sequences of words that begin with a capital letter from the given text, including specific name patterns.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to search within.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Lists for \"A. Dickes\" style sequences, multi-word capitalized sequences,\n",
    "           names with geographical or familial prefixes, sequences with titles, and general capitalized word sequences.\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace from the text\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Existing patterns\n",
    "    pattern_abbr = r'\\b[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ]\\.\\s+[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*'\n",
    "    \n",
    "    pattern_full_name = r'\\b(?:M\\.|Mlle|Mme|Monsieur|Madame|Mademoiselle|Messieurs|Mesdames|Mesdemoiselles|Herr|Frau|Fräulein|Herren|Doktor|Dr\\.|Här|Madamm|Härn|Sr\\.|Sra\\.|Srta\\.|Don|Doña|Sres\\.|Sras\\.|Dres\\.|Veuve|Witwe|Wittfra|Viuda\\s+)?(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s?)+(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ]\\.\\s)?[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*'\n",
    "    \n",
    "    pattern_multi_word = r'\\b(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s+){2,}'\n",
    "    \n",
    "    pattern_prefix = r'\\b[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s+(?:de|van|von|de la|de los|de las)\\s+[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*'\n",
    "    \n",
    "    pattern_general = r'\\b(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s*){2,}'\n",
    "    \n",
    "    # New pattern for names with initials and surname, such as \"Monsieur H.J. Sulkers\"\n",
    "    pattern_initials_with_title = r'\\b(?:M\\.|Monsieur|Madame|Herr|Frau|Dr\\.|Doktor|Här|Madamm)\\s+[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ](?:\\.[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ])?\\.\\s*[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]+'\n",
    "\n",
    "    \n",
    "    # Find all matches for \"A. Dickes\" style sequences\n",
    "    matches_abbr = re.findall(pattern_abbr, text)\n",
    "\n",
    "    # Find all matches for full names including those with initials\n",
    "    matches_full_name = re.findall(pattern_full_name, text)\n",
    "\n",
    "    # Find all matches for multi-word capitalized sequences\n",
    "    matches_multi_word = re.findall(pattern_multi_word, text)\n",
    "\n",
    "    # Find all matches for names with prefixes like \"de\", \"van\", \"von\", \"de la\", etc.\n",
    "    matches_prefix = re.findall(pattern_prefix, text)\n",
    "\n",
    "    # Find all matches for general capitalized word sequences\n",
    "    matches_general = re.findall(pattern_general, text)\n",
    "\n",
    "    # Find all matches for pattern_initials_with_title sequences\n",
    "    matches_pattern_initials_with_title = re.findall(pattern_initials_with_title, text)\n",
    "    \n",
    "    # Combine all matches into one list for multi-word capitalized sequences\n",
    "    combined_matches_multi_word = set(matches_full_name + matches_multi_word + matches_prefix + matches_general+matches_pattern_initials_with_title)\n",
    "    \n",
    "    cr = []\n",
    "    for each_item_c in combined_matches_multi_word:\n",
    "        if len(each_item_c.split(\" \")) > 1:\n",
    "            cr.append(each_item_c)\n",
    "\n",
    "    # Remove any matches that are already included in matches_abbr to avoid duplication\n",
    "    combined_matches_multi_word = [match.strip() for match in cr if match.strip() not in matches_abbr]\n",
    "\n",
    "    return matches_abbr, combined_matches_multi_word, matches_prefix, matches_full_name, matches_general\n",
    "\n",
    "\n",
    "def clean(text_in):\n",
    "    return re.sub(r'\\s+', ' ', text_in).strip()\n",
    "\n",
    "def clean_name(name):\n",
    "    # Split the name into parts to handle the first word separately\n",
    "    name = name.replace(\"MM \", \"\")\n",
    "    parts = name.split()\n",
    "\n",
    "    # Process only the first part to clean names\n",
    "    first_part = parts[0]\n",
    "\n",
    "    # Check if the first part has multiple consecutive uppercase letters and at least one lowercase letter\n",
    "    index = 0\n",
    "    for char in first_part:\n",
    "        if char.isupper():\n",
    "            index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # If the first word has more than one consecutive uppercase letter and at least one lowercase, remove all but the last\n",
    "    if index > 1 and any(char.islower() for char in first_part):\n",
    "        first_part = first_part[index-1:]\n",
    "    \n",
    "    # Replace the first part in the list of parts\n",
    "    parts[0] = first_part\n",
    "\n",
    "    # Reassemble the cleaned parts into a single string\n",
    "    cleaned_name = ' '.join(parts).strip()\n",
    "\n",
    "    return cleaned_name\n",
    "\n",
    "# Function to use the model to check if a text is a person's name\n",
    "def is_person_name(name):\n",
    "    prompt = (f\"Determine if the following text is a person's name. \"\n",
    "              f\"Consider potential typos or cultural variations in names: '{name}'. \"\n",
    "              f\"Additionally, phrases such as 'Recueil Spécial' or 'Le Receveur' or 'Signatures Enregistré' are not names for people. \"\n",
    "              f\"If the text is highly likely a person's name, answer 'yes'. Otherwise, answer 'no'.\")\n",
    "    \n",
    "    # Tokenize and move to correct device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response on the same device\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        temperature=0.2  # Lower randomness for consistent answers\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"'yes'. Otherwise, answer 'no'.\",\"\")\n",
    "    \n",
    "    if \"Answer:\" in response:\n",
    "        answer = response.split(\"Answer:\")[-1].strip().lower()\n",
    "    else:\n",
    "        answer = response.strip().lower()\n",
    "    \n",
    "    return 'yes' in answer\n",
    "\n",
    "\n",
    "# Read the file line by line with Latin-1 encoding\n",
    "\n",
    "pathfiles = [\n",
    "    '/JDH_PAPER/extracted_company_names.txt'\n",
    "]\n",
    "\n",
    "for file_path in pathfiles:\n",
    "    \n",
    "    write = open(file_path.replace(\".txt\", \"_people.txt\"), \"w\", encoding='utf-8')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Step 1: Strip the line and split using the specified delimiter \"&&&***&&&\"\n",
    "            parts = line.strip().replace(\" MM\",\" MM \").split('&&&****&&&')\n",
    "            company = clean(parts[0])\n",
    "            print(\"company: \", company)\n",
    "            # Step 2: Extract the last item from the split parts\n",
    "            last_part = clean(parts[-1]).strip()\n",
    "            if \"wrong\" not in company.lower():\n",
    "                # Step 3: Call the function to extract sequences\n",
    "                list_abbr, list_multi_word, list_prefix, list_full_name, list_general = extract_sequences(last_part)\n",
    "    \n",
    "                # Step 4: Print the results for this line\n",
    "                print(\"list_abbr: \", list_abbr)\n",
    "                abb = \"\"\n",
    "                for each_i in range(0, len(list_abbr) - 1):\n",
    "                    list_abbr[each_i] = clean(list_abbr[each_i])\n",
    "                    abb = abb + list_abbr[each_i] + \" *_* \"\n",
    "                if len(list_abbr) > 0:\n",
    "                    abb = abb + list_abbr[-1]\n",
    "                print(\"abb: \", abb)\n",
    "    \n",
    "                print(\"list_multi_word: \", list_multi_word)\n",
    "                filtered_names = [name for name in list_multi_word if is_person_name(name.replace(\"MM \", \"\"))]\n",
    "                print(filtered_names)\n",
    "                filtered_names_1=[]\n",
    "                for Fil in filtered_names:\n",
    "                    if Fil not in  filtered_names_1:\n",
    "                        filtered_names_1.append(Fil)\n",
    "                filtered_names=filtered_names_1\n",
    "                print(\"filtered_names: \", filtered_names)\n",
    "    \n",
    "                fn = \"\"\n",
    "                for each_i in range(0, len(filtered_names) - 1):\n",
    "                    filtered_names[each_i] = clean(filtered_names[each_i])\n",
    "                    filtered_names[each_i] = clean_name(filtered_names[each_i])\n",
    "                    fn = fn + filtered_names[each_i] + \" *_* \"\n",
    "                if len(filtered_names) > 0:\n",
    "                    fn = fn + filtered_names[-1]\n",
    "            else:\n",
    "                fn=\"\"\n",
    "                abb=\"\"\n",
    "            print(\"fn: \", fn)\n",
    "            print(\"***************************************************************************\")\n",
    "\n",
    "            fn = clean(fn)\n",
    "            abb = clean(abb)\n",
    "\n",
    "            print(\"******************\")\n",
    "            newline = company.replace('\"',\"\") + \" &&&***&&& \" + company.replace('\"',\"\") + \" &&&***&&& \" + fn + \" &&&***&&& \" + abb + \" &&&***&&& \" +\\\n",
    "            last_part + \"\\n\"\n",
    "            write.write(newline)\n",
    "            write.flush()\n",
    "\n",
    "            print(\"-\" * 40)  # Separator for output clarity\n",
    "    write.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next explore another key component of the overall implementation: a pipeline for fine-tuning a GPT-2 model with enhanced capabilities for efficient training, response generation, and model merging. This section complements the previous modules by introducing methods for adapting large language models to specific tasks using Low-Rank Adaptation (LoRA), managing system resources effectively, and integrating fine-tuned parameters into base models for flexible deployment. The process begins with class initialization, where core attributes such as the dataset path, cache directory, and Hugging Face authentication token are defined. A custom GPT-2 configuration is established by adjusting architecture parameters like hidden size, attention heads, number of layers, vocabulary size, and activation function. Using this configuration, the tokenizer and model are loaded from Hugging Face, and the dataset—assumed to be in JSON format—is imported and prepared using the datasets library.\n",
    "Fine-tuning is carried out using LoRA, which allows selective updating of specific layers in the model to reduce training overhead. Training parameters, such as epoch count, batch size, and logging intervals, are specified and passed to a supervised fine-tuning trainer, which handles the model training process. The fine-tuned model is saved to a designated output directory, with training metrics optionally tracked via TensorBoard.\n",
    "For inference, the pipeline includes a response generation method that constructs a prompt, tokenizes it, and produces a concise answer using the fine-tuned model. Generation behavior is controlled through parameters like output length and temperature. The output is then decoded and cleaned to return a focused response. Resource management is addressed through a cleanup function that removes model components from memory and clears the GPU cache. Additionally, the code supports selective model merging, which involves loading both the base and fine-tuned models, comparing their parameters, and updating the base model with selected weights—ensuring compatibility before saving the merged model. This module fits into the larger architecture as a reusable and scalable fine-tuning pipeline that supports both training and inference. The following code block provides the full implementation of this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    GPT2Config  # Import the GPT-2 configuration class\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from random import randrange\n",
    "from trl import SFTTrainer  # Correct import for the SFTTrainer\n",
    "\n",
    "class FineTune_GPT2:\n",
    "    def __init__(self, dataset_path, cache_dir, token):\n",
    "        \"\"\"\n",
    "        Initialize the FineTune_GPT2 class.\n",
    "\n",
    "        Args:\n",
    "            dataset_path (str): Path to the dataset.\n",
    "            cache_dir (str): Directory to cache the models and tokenizers.\n",
    "            token (str): Hugging Face API token for accessing gated repositories.\n",
    "\n",
    "        How to obtain the Hugging Face API token:\n",
    "        1. Go to https://huggingface.co/settings/tokens\n",
    "        2. Log in or sign up if you don't have an account.\n",
    "        3. Create a new token with the required permissions (read access is sufficient).\n",
    "        4. Copy the token and provide it when initializing this class.\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.cache_dir = cache_dir\n",
    "        self.token = token\n",
    "\n",
    "        # Embedded configuration\n",
    "        config_dict = {\n",
    "            \"architectures\": [\"GPT2LMHeadModel\"],\n",
    "            \"bos_token_id\": 50256,\n",
    "            \"eos_token_id\": 50256,\n",
    "            \"hidden_act\": \"gelu_new\",\n",
    "            \"hidden_size\": 768,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"intermediate_size\": None,\n",
    "            \"layer_norm_eps\": 1e-05,\n",
    "            \"model_type\": \"gpt2\",\n",
    "            \"num_attention_heads\": 12,\n",
    "            \"num_hidden_layers\": 12,\n",
    "            \"vocab_size\": 50257\n",
    "        }\n",
    "        self.config = GPT2Config.from_dict(config_dict)  # Use the correct configuration class\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"gpt2\", \n",
    "            cache_dir=self.cache_dir, \n",
    "            use_auth_token=self.token  # Use the correct parameter\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Set padding token to eos token\n",
    "        self.tokenizer.padding_side = 'right'  # Ensure padding is on the right\n",
    "        self.trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"gpt2\", \n",
    "            config=self.config,\n",
    "            cache_dir=self.cache_dir,\n",
    "            use_auth_token=self.token  # Use the correct parameter\n",
    "        ).to(\"cuda\")\n",
    "        self.dataset = load_dataset('json', data_files=self.dataset_path, split='train')\n",
    "\n",
    "    def train_model(self, output_dir, num_train_epochs=3, per_device_train_batch_size=2, per_device_eval_batch_size=1, max_seq_length=None):\n",
    "        lora_config = LoraConfig(\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            r=64,\n",
    "            target_modules=[\n",
    "                \"attn.c_attn\",\n",
    "                \"attn.c_proj\",\n",
    "                \"mlp.c_fc\",\n",
    "                \"mlp.c_proj\",\n",
    "            ],\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=10000,\n",
    "            save_steps=1000,\n",
    "            warmup_ratio=0.03,\n",
    "            report_to=\"tensorboard\"\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.trained_model,\n",
    "            train_dataset=self.dataset,\n",
    "            peft_config=lora_config,\n",
    "            dataset_text_field=\"text\",  # Assuming 'text' is the field name containing the text data\n",
    "            max_seq_length=max_seq_length,  # Pass None or specify a maximum sequence length\n",
    "            tokenizer=self.tokenizer,\n",
    "            args=training_args\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "        print(\"The new model is available in \" + output_dir)\n",
    "        self.trained_model = trainer.model\n",
    "\n",
    "    def generate_response(self, question, max_new_tokens=500, temperature=0.1):\n",
    "        prompt = f\"\"\"You will be provided with a question. You must provide only a single answer. You must not provide additional questions and answers.\n",
    "        Question:\n",
    "        {question}\n",
    "        \"\"\"\n",
    "        model_input = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            generated_code = self.trained_model.generate(**model_input, max_new_tokens=max_new_tokens, pad_token_id=self.tokenizer.eos_token_id, temperature=temperature)\n",
    "            generated_code = self.tokenizer.decode(generated_code[0], skip_special_tokens=True)\n",
    "            response = generated_code.split(\"You will be provided with a question\")[1]\n",
    "            if len(response) < 10:\n",
    "                return generated_code\n",
    "        return response\n",
    "\n",
    "    def clean_up(self):\n",
    "        del self.tokenizer\n",
    "        del self.trained_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def selective_merge(self, base_model_path, fine_tuned_model_path, output_dir):\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model_path, cache_dir=self.cache_dir, use_auth_token=self.token).to(\"cuda\")\n",
    "        fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path, cache_dir=self.cache_dir, use_auth_token=self.token).to(\"cuda\")\n",
    "\n",
    "        # Extract state dicts\n",
    "        base_state_dict = base_model.state_dict()\n",
    "        ft_state_dict = fine_tuned_model.state_dict()\n",
    "\n",
    "        # Filter out keys: only update base model with keys that exist in its state dict and have the same size\n",
    "        for key in ft_state_dict:\n",
    "            if key in base_state_dict and ft_state_dict[key].size() == base_state_dict[key].size():\n",
    "                base_state_dict[key] = ft_state_dict[key]\n",
    "\n",
    "        # Load the filtered state dict back into the base model\n",
    "        base_model.load_state_dict(base_state_dict, strict=False)\n",
    "\n",
    "        # Save the merged model\n",
    "        base_model.save_pretrained(output_dir)\n",
    "\n",
    "        # Save tokenizer and configuration as well\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        self.config.save_pretrained(output_dir)\n",
    "\n",
    "        print(f\"Merged model, tokenizer, and config are saved in {output_dir}\")\n",
    "\n",
    "        return base_model\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate and use the FineTune_GPT2 class\n",
    "dataset_path = \"/JDH_PAPER/training.jsonl\"\n",
    "hf_token = \".............\" # your token\n",
    "cache_dir = \"/JDH_PAPER\"\n",
    "output_dir = \"/JDH_PAPER/Models/\"\n",
    "merged_model_output_dir = \"/JDH_PAPER/Merged\"\n",
    "\n",
    "# Create an instance of FineTune_GPT2 and start training\n",
    "fine_tuner = FineTune_GPT2(dataset_path, cache_dir, token=hf_token)\n",
    "\n",
    "\n",
    "# Train the model and save it to the output directory\n",
    "fine_tuner.train_model(output_dir=output_dir, num_train_epochs=25, per_device_train_batch_size=2, per_device_eval_batch_size=1)\n",
    "\n",
    "# Optionally, merge the fine-tuned model with the base model\n",
    "fine_tuner.selective_merge(base_model_path=output_dir, fine_tuned_model_path=output_dir, output_dir=merged_model_output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the previous components, this section demonstrates how a fine-tuned GPT-2 model is used to extract addresses from unstructured text through a complete pipeline of preprocessing, inference, and post-processing. The process begins with initializing a class that loads the tokenizer and model from a specified directory, using a Hugging Face token and memory-efficient settings such as half-precision (torch.float16) and right-side padding for token alignment.\n",
    "To guide the model’s output, the code constructs a question-style prompt, which is tokenized and passed through the model using controlled generation parameters like maximum token length and temperature for deterministic responses. The output is then decoded and cleaned to remove special tokens or redundant text.\n",
    "Input is read from a structured file where each line contains company data. Each line is split using a custom delimiter, and the relevant portion is extracted to form the prompt. The generated response is then processed using regex to isolate the address, clean it, and apply fallback logic in cases where a valid address cannot be determined.\n",
    "Following address extraction, the output undergoes an additional cleaning stage to enhance readability, and results are written to a new file with consistent formatting. The code includes error-handling procedures that reinitialize the model if failures occur and employs memory management techniques such as garbage collection and GPU cache clearing.\n",
    "This module integrates seamlessly into the larger system, handling input, generating and refining address data, and producing clean, structured output ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import torch\n",
    "import os,re\n",
    "\n",
    "\n",
    "class FineTune_GPT2_use:\n",
    "    def __init__(self, cache_dir=None, token=None, model_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the class with cache_dir, token, and model_path for the GPT2 model.\n",
    "        \"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        self.token = token\n",
    "\n",
    "        # Load the tokenizer and model from the specified path (fine-tuned model folder)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=self.cache_dir, token=self.token)\n",
    "\n",
    "        # Load the model on CPU for debugging, or on CUDA with float16 if memory is an issue\n",
    "        self.trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, \n",
    "            cache_dir=self.cache_dir, \n",
    "            token=self.token, \n",
    "            torch_dtype=torch.float16  # Try using float16 to reduce memory\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Set padding token for the tokenizer\n",
    "        # self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = 'right'\n",
    "\n",
    "    def generate_response(self, question, max_new_tokens=500, temperature=0.1):\n",
    "        \"\"\"\n",
    "        Generate a response to the given question using the fine-tuned GPT2 model.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to ask the model.\n",
    "            max_new_tokens (int): Maximum number of tokens to generate.\n",
    "            temperature (float): The temperature for generation.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Tokenize the input and print for debugging\n",
    "        inputs = self.tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        inputs = inputs.to(device)  # Move inputs to the same device as the model\n",
    "\n",
    "        # Generate text with no gradient computation\n",
    "        with torch.no_grad():\n",
    "            generated_output = self.trained_model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=max_new_tokens, \n",
    "                pad_token_id=self.tokenizer.eos_token_id, \n",
    "                temperature=temperature\n",
    "            )\n",
    "            # Decode the generated text and remove special tokens\n",
    "            generated_text = self.tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    def clean_up(self):\n",
    "        \"\"\"\n",
    "        Clean up the resources by deleting the model and tokenizer, and clearing the GPU cache.\n",
    "        \"\"\"\n",
    "        del self.tokenizer\n",
    "        del self.trained_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Run the model on CPU or GPU with debug options to find the problem\n",
    "if __name__ == \"__main__\":\n",
    "    gpt2_model_path = \"/JDH_PAPER/Merged/\"\n",
    "    fine_tuner1 = FineTune_GPT2_use(cache_dir='/JDH_PAPER', token='use_your_token', model_path=gpt2_model_path)\n",
    "    pathfiles = [\n",
    "        '/JDH_PAPER/extracted_company_names_people.txt'\n",
    "    ]\n",
    "#     question = \"Where is the address of the company in the following text?\"\n",
    "#     response = fine_tuner.generate_response(question)\n",
    "#     print(\"response:\",response)\n",
    "    for each_fine in pathfiles:\n",
    "        dataset_path = each_fine.replace(\"_people.txt\", \"_people_Addr.txt\")\n",
    "        write = open(dataset_path, \"w\", encoding='utf-8')\n",
    "        read_file = each_fine\n",
    "\n",
    "        read = open(read_file, 'r', encoding='utf-8')\n",
    "        for line in read:\n",
    "            # Process each line and extract relevant information\n",
    "            L = line.split(\" &&&***&&& \")\n",
    "            L[-1]=L[-1].rstrip().lstrip().strip()\n",
    "            CopyL=L[-1]\n",
    "            L[-1] = L[-1].split(\"+++++++++++++++++\")[-1].replace(\" : \", \": \")\n",
    "\n",
    "            # Step 1: Use GPT-2 fine-tuned model to find address in text\n",
    "            te=\"\"\n",
    "            size_L=50\n",
    "            if len(L[-1].split(\" \"))<50:\n",
    "                size_L=len(L[-1].split(\" \"))\n",
    "            for e in range(0,size_L):\n",
    "                te=te+L[-1].split(\" \")[e]+\" \"\n",
    "            question=\"Where is the address of the company in the following text? \"+te \n",
    "            print(\"question: \",question)\n",
    "            try:\n",
    "                address=fine_tuner1.generate_response(question)\n",
    "            except:\n",
    "                del fine_tuner1\n",
    "                fine_tuner1 = FineTune_GPT2_use(cache_dir='/JDH_PAPER/', token='your_token', model_path=gpt2_model_path)\n",
    "\n",
    "            address = re.sub(r'\\s+', ' ', address)  # Clean the response\n",
    "            \n",
    "            try:\n",
    "                # Extract address, handle missing or incorrect data gracefully\n",
    "                address = re.sub(r'\\s+', ' ', address).split(\"Answer:\")[1].replace(\"The Address is :\", \"\").replace(\"</s>\", \"\").replace(\"</s\", \"\").replace(\"/s>\", \"\")\n",
    "            except:\n",
    "                print(\"To be checked: \", address)\n",
    "                address = \"No Addr\"\n",
    "\n",
    "            address = re.sub(r'\\s+', ' ', address)\n",
    "            address=re.sub(r\"(?<!^)(?=[A-Z])\", \" \", address)\n",
    "            address = re.sub(r'\\s+', ' ', address)  # Clean the response\n",
    "            if len(address)>120:\n",
    "                address=\"No Addr\"\n",
    "\n",
    "            refined_address = address  # Refinement step could be added here\n",
    "            print(\"++++++++++\")\n",
    "            print(\"refined address:\", refined_address)\n",
    "            print(\"++++++++++\")\n",
    "            print(\"=====================================================================\")\n",
    "\n",
    "            # Write results back to the file\n",
    "            newline = L[0]+\" &&&***&&& \"+L[1]+\" &&&***&&& \"\\\n",
    "            +L[2]+\" &&&***&&& \"+L[3]+\" &&&***&&& \"+refined_address+\" &&&***&&& \"+CopyL.replace('\\n', '')+\"\\n\"\n",
    "\n",
    "            write.write(newline)\n",
    "            write.flush()\n",
    "\n",
    "        write.close()\n",
    "        read.close()\n",
    "\n",
    "        # Clean up resources after processing\n",
    "        fine_tuner1.clean_up()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the capabilities of the pipeline, this module focuses on identifying and extracting country names from text using a multilingual dictionary and regex-based matching. A predefined dictionary maps country names and their variations in English, French, German, and Luxembourgish. The code reads an input file containing company information, splits each line into structured components using a custom delimiter, and applies text cleaning to normalize whitespace. For each line, it scans the relevant text component—typically a description or address—against all known country name variations. A for loop is used to match standalone country names while avoiding partial matches, with special handling to distinguish cases like “Jersey” and “New Jersey” based on frequency. Extracted country names are deduplicated and formatted using a custom separator before being appended to the original data. The processed results are written to a new output file in a structured format, ensuring compatibility with downstream tasks. Throughout execution, the code prints intermediate values such as company names, matched text, and identified countries for verification. This module adds robust, multilingual country name extraction to the processing pipeline, with careful handling of edge cases and clear formatting for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/JDH_PAPER/extracted_company_names_people_Addr.txt\"\n",
    "read=open(file,\"r\",encoding=\"utf-8\")\n",
    "write=open(\"/JDH_PAPER/final_a.txt\",\"w\",encoding=\"utf-8\")\n",
    "import re\n",
    "for line in read:\n",
    "    L=line.split(\"&&&***&&&\")\n",
    "    names_c=[]\n",
    "    for i in range(0, len(L)):\n",
    "        L[i] = re.sub(r'\\s+', ' ', L[i]).strip()\n",
    "    string_name_co=\" \"\n",
    "    if \"wrong\" in L[0].lower():\n",
    "        pass\n",
    "    else:\n",
    "        found = set()\n",
    "        for english_name, variants in countries.items():\n",
    "            all_variants = set(variant.lower() for variant in variants + [english_name])\n",
    "    \n",
    "            if english_name == \"Jersey\":\n",
    "                if is_clean_match(normalized_text, \"jersey\") and not is_clean_match(normalized_text, \"new jersey\"):\n",
    "                    found.add(\"Jersey\")\n",
    "                    JJCC += 1\n",
    "            else:\n",
    "                for variant in all_variants:\n",
    "                    if is_clean_match(normalized_text, variant):\n",
    "                        found.add(english_name)\n",
    "\n",
    "        names_c=list(found)\n",
    "        for e in names_c:\n",
    "            string_name_co=string_name_co+e+\" *_* \"\n",
    "        string_name_co=re.sub(r'\\s+', ' ', string_name_co).strip()[:-4]\n",
    "    newline=\"\"\n",
    "    for i in range(0,len(L)-1):\n",
    "       newline=newline+L[i]+\" &&&***&&& \" \n",
    "    newline=newline+string_name_co+\" &&&***&&& \" +L[-1]\n",
    "    newline=re.sub(r'\\s+', ' ', newline).strip()\n",
    "    print(\"Company: \\n \",L[0])\n",
    "    print(\"Description: \\n \",L[-1].split(\"+++++++++++++++++\")[-1])\n",
    "    print(\"++++++++\")\n",
    "    print(\"Countries:\")\n",
    "    print(string_name_co)\n",
    "    print(\"++++++++\")\n",
    "\n",
    "    print(\"****************************************************************************************************\")\n",
    "\n",
    "    write.write(newline.replace(\"\\n\",\"\")+\"\\n\")\n",
    "    write.flush()\n",
    "write.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the people extraction pipeline, an additional improvement step is introduced to enhance data consistency by removing formal titles from extracted person names. This refinement ensures that names are cleaned before entering the loading phase, eliminating job roles, honorifics, and linguistic variations that can introduce noise in downstream analysis. The cleaning process begins with a function to remove excess whitespace, using regular expressions to replace multiple spaces with a single space and trimming any leading or trailing characters. The core of the enhancement lies in the title removal function, which relies on a comprehensive, multilingual list of titles, including English, French, German, Luxembourgish, Spanish, and Portuguese variants. A flexible regex pattern is constructed to match these titles as standalone words, accounting for optional punctuation and accented characters. Matches are removed in a case-insensitive and Unicode-aware manner to ensure robustness across languages. Once processed, the names are returned in a clean, title-free format. For example, inputs such as “Dr. John Doe,” “Monsieur Jean Dupont,” or “Herr Karl Müller” are standardised to “John Doe,” “Jean Dupont,” and “Karl Müller,” respectively. This step significantly improves the uniformity and accuracy of person name data, simplifies further processing, and adapts easily to multilingual contexts. Integrated directly before the loading stage, this module ensures that only properly cleaned names are included in the final dataset, contributing to overall data quality and reliability within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    # Use regular expression to replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading and trailing spaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text\n",
    "    \n",
    "def remove_titles(name):\n",
    "    import re\n",
    "\n",
    "    # List of common titles in English, French, German, Luxembourgish, Spanish, and Portuguese\n",
    "    titles = [\n",
    "        # English Titles\n",
    "        r'\\bMr\\b', r'\\bMrs\\b', r'\\bMs\\b', r'\\bMiss\\b', r'\\bDr\\b', r'\\bProf\\b', r'\\bSir\\b',\n",
    "        r'\\bMadam\\b', r'\\bLord\\b', r'\\bLady\\b', r'\\bRev\\b', r'\\bHon\\b', r'\\bJudge\\b',\n",
    "        r'\\bDame\\b', r'\\bCapt\\b', r'\\bCol\\b', r'\\bGen\\b', r'\\bLt\\b', r'\\bMaj\\b',\n",
    "        r'\\bSgt\\b', r'\\bCpl\\b', r'\\bPvt\\b', r'\\bChief\\b', r'\\bOfficer\\b', r'\\bDetective\\b',\n",
    "        r'\\bAttorney\\b', r'\\bAmb\\b', r'\\bConsul\\b', r'\\bSec\\b', r'\\bDir\\b', r'\\bMgr\\b',\n",
    "        r'\\bCEO\\b', r'\\bCFO\\b', r'\\bCTO\\b', r'\\bPresident\\b', r'\\bVP\\b', r'\\bDep\\b',\n",
    "\n",
    "        # French Titles\n",
    "        r'\\bMonsieur\\b', r'\\bMadame\\b', r'\\bMademoiselle\\b', r'\\bMlle\\b', r'\\bMme\\b', r'\\bMaître\\b',r'\\bMaitre\\b',r'\\bTissus\\b',\n",
    "        r'\\bDocteur\\b', r'\\bProfesseur\\b', r'\\bPrésident\\b', r'\\bVice-président\\b', r'\\bSecrétaire\\b',r'\\bLuxembourg\\b',r'\\bMaitre\\b',\n",
    "        r'\\bDirecteur\\b', r'\\bPDG\\b', r'\\bAdministrateur\\b', r'\\bComte\\b', r'\\bComtesse\\b', r'\\bComptes\\b',r'\\bBelgique\\b'\n",
    "        r'\\bMarquis\\b', r'\\bMarquise\\b', r'\\bDuc\\b', r'\\bDuchesse\\b', r'\\bPrince\\b', r'\\bPrincesse\\b',\n",
    "        r'\\bMïster\\b', r'\\bMessieurs\\b', # Mister with an ï (for completeness)\n",
    "\n",
    "        # German Titles\n",
    "        r'\\bHerr\\b', r'\\bFrau\\b', r'\\bFräulein\\b', r'\\bHr\\b', r'\\bFr\\b', r'\\bDoktor\\b',\n",
    "        r'\\bProfessor\\b', r'\\bPräsident\\b', r'\\bVizepräsident\\b', r'\\bSekretär\\b',\n",
    "        r'\\bDirektor\\b', r'\\bVerwalter\\b', r'\\bKanzler\\b', r'\\bBaron\\b', r'\\bBaronin\\b',\n",
    "        r'\\bGraf\\b', r'\\bGräfin\\b', r'\\bFürst\\b', r'\\bFürstin\\b', r'\\bHerzog\\b', r'\\bHerzogin\\b',\n",
    "\n",
    "        # Luxembourgish Titles\n",
    "        r'\\bHär\\b', r'\\bMadame\\b', r'\\bMademoiselle\\b', r'\\bDokter\\b', r'\\bProfesser\\b',\n",
    "        r'\\bBuergermeeschter\\b', r'\\bPrinz\\b', r'\\bPrinzessin\\b', r'\\bGroßherzog\\b', r'\\bGroßherzogin\\b',\n",
    "\n",
    "        # Spanish Titles\n",
    "        r'\\bSeñor\\b', r'\\bSeñora\\b', r'\\bSeñorita\\b', r'\\bDr\\b', r'\\bDra\\b', r'\\bProf\\b', r'\\bProfa\\b',\n",
    "        r'\\bDon\\b', r'\\bDoña\\b', r'\\bLicenciado\\b', r'\\bLicenciada\\b', r'\\bIngeniero\\b',\n",
    "        r'\\bIngeniera\\b', r'\\bPresidente\\b', r'\\bVicepresidente\\b', r'\\bDirector\\b',\n",
    "        r'\\bAdministradora\\b', r'\\bAlcalde\\b', r'\\bAlcaldesa\\b',\n",
    "\n",
    "        # Portuguese Titles\n",
    "        r'\\bSenhor\\b', r'\\bSenhora\\b', r'\\bSenhorita\\b', r'\\bDoutor\\b', r'\\bDoutora\\b',\n",
    "        r'\\bProfessor\\b', r'\\bProfessora\\b', r'\\bEngenheiro\\b', r'\\bEngenheira\\b',\n",
    "        r'\\bPresidente\\b', r'\\bVice-presidente\\b', r'\\bDiretor\\b', r'\\bDiretora\\b',\n",
    "        r'\\bAdministrador\\b', r'\\bAdministradora\\b', r'\\bPrefeito\\b', r'\\bPrefeita\\b'\n",
    "    ]\n",
    "\n",
    "    # Create a regex pattern that matches any of the titles (including accented characters)\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(titles) + r')\\.?', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    # Substitute the titles with an empty string\n",
    "    name_without_titles = re.sub(pattern, '', name).strip()\n",
    "    return name_without_titles\n",
    "import re\n",
    "read=open(\"/JDH_PAPER/final_a.txt\",\"r\",encoding=\"utf-8\")\n",
    "write=open(\"/JDH_PAPER/final_a.txt\".replace(\"_a.txt\",\"_c.txt\"),\"w\",encoding=\"utf-8\")\n",
    "\n",
    "delimiter=\"&&&***&&&\"\n",
    "count=0\n",
    "for line in read:\n",
    "    L=line.split(delimiter)\n",
    "    L[2]=remove_extra_spaces(L[2]).rstrip().lstrip().strip()\n",
    "    print(\"Input names: \",L[2])\n",
    "    print()\n",
    "    newL2=L[2].split(\" *_* \")\n",
    "\n",
    "    PEOPLE=[]\n",
    "    output_name=[]\n",
    "    for index in range(0,len(newL2)):\n",
    "        newL2[index]=remove_titles(remove_extra_spaces(newL2[index]).lstrip().rstrip().strip())\n",
    "        if len(newL2[index].split(\" \"))>1:\n",
    "            if len(newL2[index].split(\" \")[-1])>2:\n",
    "                output_name.append(remove_extra_spaces(newL2[index]).lstrip().rstrip().strip())\n",
    "    print(\"Cleaned names: \",output_name)\n",
    "    newline=\"\"\n",
    "    for i in range(0,len(L)-1):\n",
    "        if i!=2:\n",
    "            newline=newline+L[i]+\" \"+delimiter+\" \"\n",
    "\n",
    "        else:\n",
    "            output_string=\"\"\n",
    "            for each_n in output_name:\n",
    "                output_string=output_string+each_n+\" *_* \"\n",
    "            output_string=output_string[:-4].lstrip().rstrip().strip()\n",
    "            print(\"output_string:\",output_string)\n",
    "            newline=newline+output_string+\" \"+delimiter+\" \"\n",
    "    newline=newline+L[-1]\n",
    "    write.write(newline)\n",
    "    write.flush()\n",
    "    print()\n",
    "    print(\"*************************************************\")\n",
    "read.close()\n",
    "write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the ETL pipeline, the final step involves loading the processed and cleaned data into a relational database, ensuring both data integrity and the establishment of meaningful relationships between entities such as companies, addresses, individuals, countries, and messages. This stage begins by connecting to a SQL Server database using pyodbc, with secure authentication and error handling to validate connectivity. For efficiency, the implementation preloads existing records into in-memory dictionaries, reducing redundant queries by caching entity IDs. Before insertion, values are standardized using cleaning functions to ensure consistency. Helper methods manage the insertion or retrieval of primary keys by first checking for existing entries and inserting new ones only when necessary. These keys are then used to establish associations across multiple junction tables, supporting complex many-to-many relationships. The pipeline processes an input file line by line, splitting each record into structured components using a custom delimiter. Company names, alternate names, addresses, person names, and countries are extracted, cleaned, and matched against the database. When a match is found or inserted, corresponding IDs are retrieved and used to link data across the relational schema. Descriptions are recorded in a messages table and connected to relevant entities via junction tables. Special handling is included to manage address length limits and ambiguous company names.The code dynamically adjusts to different file encodings and logs errors during any part of the process—such as file reading, database operations, or line parsing—ensuring that individual failures do not interrupt the overall workflow. Upon successful processing, the data is committed to the database, and all resources are properly released.By preventing duplicates, maintaining clean formatting, and efficiently linking entities, this step finalizes the ETL pipeline with a scalable and resilient approach to structured data integration, enabling robust querying and downstream analytical tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Define your connection string with the correct credentials and server information\n",
    "conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.184.4.29;DATABASE=LETTER;UID=c2dh;PWD=C2dh4ever'\n",
    "\n",
    "# Establish the connection\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connection successful\")\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error connecting to the database: {e}\")\n",
    "    raise\n",
    "\n",
    "# List of tables to delete from\n",
    "# Note: Order matters - delete from the junction tables first to avoid foreign key constraint violations\n",
    "tables = [\n",
    "    'People_Addresses',\n",
    "    'People_Companies',\n",
    "    'People_Countries',\n",
    "    'People_MessagesL',\n",
    "    'Addresses_Countries',\n",
    "    'Companies_Countries',\n",
    "    'People',\n",
    "    'Addresses',\n",
    "    'Companies',\n",
    "    'Countries',\n",
    "    'MessagesL',\n",
    "]\n",
    "\n",
    "# Function to delete all rows from the tables\n",
    "def delete_all_data():\n",
    "    try:\n",
    "        for table in tables:\n",
    "            print(f\"Deleting data from {table}...\")\n",
    "            cursor.execute(f\"DELETE FROM {table}\")  # Using DELETE to respect foreign key constraints\n",
    "            conn.commit()\n",
    "            print(f\"Data deleted from {table}\")\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error deleting data from {table}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run the delete process\n",
    "delete_all_data()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection closed\")\n",
    "import pyodbc\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "\n",
    "def has_column(table, column):\n",
    "    \"\"\"\n",
    "    Check if a column exists in a specific table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT 1\n",
    "            FROM INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE TABLE_NAME = ? AND COLUMN_NAME = ?\n",
    "        \"\"\", table, column)\n",
    "        return cursor.fetchone() is not None\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error checking column {column} in table {table}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def assign_unused_id(table, id_column):\n",
    "    \"\"\"\n",
    "    Retrieve the next unused ID for a given table and column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT MAX({id_column}) FROM {table}\")\n",
    "        max_id = cursor.fetchone()[0]\n",
    "        return (max_id or 0) + 1\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error fetching max ID from {table}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def insert_or_get_id(table, column, value, year=None, id_column=None):\n",
    "    \"\"\"\n",
    "    Insert a new record or retrieve the ID of an existing record.\n",
    "    \"\"\"\n",
    "\n",
    "    if not id_column:\n",
    "        id_column = f\"{table[:-1]}_id\"\n",
    "\n",
    "    try:\n",
    "        # Check if the record already exists\n",
    "        query = f\"SELECT {id_column} FROM {table} WHERE {column} = ?\"\n",
    "        params = [value]\n",
    "        if has_column(table, \"year\"):\n",
    "            query += \" AND year = ?\"\n",
    "            params.append(year)\n",
    "\n",
    "        cursor.execute(query, params)\n",
    "        row = cursor.fetchone()\n",
    "\n",
    "        # Return the ID if the record exists\n",
    "        if row:\n",
    "            return row[0]\n",
    "\n",
    "        # Assign new ID\n",
    "        new_id = assign_unused_id(table, id_column)\n",
    "\n",
    "        # Insert new record\n",
    "        columns = [id_column, column]\n",
    "        values = [new_id, value]\n",
    "        placeholders = [\"?\", \"?\"]\n",
    "\n",
    "        if has_column(table, \"year\") and year is not None:\n",
    "            columns.append(\"year\")\n",
    "            values.append(year)\n",
    "            placeholders.append(\"?\")\n",
    "\n",
    "        if has_column(table, \"effective_start_date\"):\n",
    "            columns.append(\"effective_start_date\")\n",
    "            placeholders.append(\"?\")\n",
    "            values.append(\"1970-01-01\")  # Default start date\n",
    "\n",
    "        if has_column(table, \"is_current\"):\n",
    "            columns.append(\"is_current\")\n",
    "            placeholders.append(\"?\")\n",
    "            values.append(1)  # Default value for is_current\n",
    "\n",
    "        if has_column(table, \"current_version\"):\n",
    "            columns.append(\"current_version\")\n",
    "            placeholders.append(\"?\")\n",
    "            values.append(1)  # Default version for new records\n",
    "\n",
    "        insert_query = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(placeholders)})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "        conn.commit()\n",
    "\n",
    "        return new_id\n",
    "\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error inserting or retrieving ID from {table} for value '{value}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def insert_into_junction(table, col1, col2, year, id1, id2):\n",
    "    \"\"\"\n",
    "    Insert or update a relationship in a junction table using SCD Type 2 logic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if an active record exists\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT is_current, effective_start_date, effective_end_date\n",
    "            FROM {table}\n",
    "            WHERE {col1} = ? AND {col2} = ? AND year = ?\n",
    "        \"\"\", id1, id2, year)\n",
    "        row = cursor.fetchone()\n",
    "\n",
    "        if row and row[0] == 1:  # Active record exists\n",
    "            return  # No need to update\n",
    "\n",
    "        if row:  # Inactivate the existing record\n",
    "            cursor.execute(f\"\"\"\n",
    "                UPDATE {table}\n",
    "                SET is_current = 0, effective_end_date = GETDATE()\n",
    "                WHERE {col1} = ? AND {col2} = ? AND year = ? AND is_current = 1\n",
    "            \"\"\", id1, id2, year)\n",
    "            conn.commit()\n",
    "\n",
    "        # Insert the new record\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO {table} ({col1}, {col2}, year, is_current, effective_start_date, effective_end_date)\n",
    "            VALUES (?, ?, ?, 1, GETDATE(), NULL)\n",
    "        \"\"\", id1, id2, year)\n",
    "        conn.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error inserting into {table} (col1={col1}, col2={col2}, year={year}): {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def process_file(path_file):\n",
    "    \"\"\"\n",
    "    Process the input file and populate the database with its data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        line_number=0\n",
    "        with open(path_file, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Split the line into columns\n",
    "                columns = line.strip().split(' &&&***&&& ')\n",
    "                # Extract the data from the line\n",
    "                year = int(columns[0].strip())\n",
    "                company_name = columns[1]\n",
    "                address = columns[3]\n",
    "                \n",
    "                country_name = columns[7].split(\"*_*\")\n",
    "                for index_country in range(0,len(country_name)):\n",
    "                    country_name[index_country]=country_name[index_country].rstrip().lstrip().strip()\n",
    "                \n",
    "                person_name = columns[5].split(\"*_*\")\n",
    "                for person_index in range(0,len(person_name)):\n",
    "                    person_name[person_index]=person_name[person_index].rstrip().lstrip().strip()\n",
    "\n",
    "                try:\n",
    "                    person_name.remove('')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    person_name.remove('')\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if \"wrong\" in company_name.lower():\n",
    "                    continue\n",
    "                else:\n",
    "                # Insert or retrieve the IDs\n",
    "                    country_ids=[]\n",
    "                    person_ids=[]\n",
    "                    \n",
    "                    company_id = insert_or_get_id('Companies', 'company_name', company_name, year, 'company_id')\n",
    "                    address_id = insert_or_get_id('Addresses', 'address_description', address, year, 'address_id')\n",
    "\n",
    "                    for country in country_name:\n",
    "                        country_id = insert_or_get_id('Countries', 'country_name', country, year, 'country_id')\n",
    "                        country_ids.append(country_id)\n",
    "                            \n",
    "                    for person in person_name:\n",
    "                        person_id = insert_or_get_id('People', 'person_name', person, year, 'person_id')\n",
    "                        person_ids.append(person_id)\n",
    "                            \n",
    "                    # Insert into junction tables\n",
    "                    for country_id in country_ids:\n",
    "                        if company_id and country_id:\n",
    "                            print(\"company_id:\",company_id)\n",
    "                            print(\"country_id:\",country_id)\n",
    "                            insert_into_junction('Companies_Countries', 'company_id', 'country_id', year, company_id, country_id)\n",
    "                        if address_id and country_id:\n",
    "                            insert_into_junction('Addresses_Countries', 'address_id', 'country_id', year, address_id, country_id)\n",
    "\n",
    "                    for person_id in person_ids:\n",
    "                        if person_id and company_id:\n",
    "                            insert_into_junction('People_Companies', 'person_id', 'company_id', year, person_id, company_id)\n",
    "                        if person_id and address_id:\n",
    "                            insert_into_junction('People_Addresses', 'person_id', 'address_id', year, person_id, address_id)\n",
    "\n",
    "                    \n",
    "                    print(\"line_number:\",line_number)\n",
    "                line_number=line_number+1\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path_file}\")\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Database error while processing file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.184.4.29;DATABASE=LETTER;UID=c2dh;PWD=C2dh4ever'\n",
    "\n",
    "    for year in range(1970, 1971):\n",
    "        try:\n",
    "            conn = pyodbc.connect(conn_str)\n",
    "            cursor = conn.cursor()\n",
    "            print(f\"Processing data for year {year}...\")\n",
    "            path_file = f\"../FILE_IMPORT/{year}_final_2ADDR_final_check_c.txt\"\n",
    "            process_file(path_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            try:\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "citation-manager": {
   "items": {}
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
