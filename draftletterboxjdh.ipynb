{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"celltoolbar":"Tags","citation-manager":{"items":{}},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title","metadata":{"slideshow":{"slide_type":""},"tags":["title"]}},{"cell_type":"markdown","source":" ### Benoît  Majerus [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0003-4869-2061) \nCentre for Contemporary and Digital History, University of Luxembourg","metadata":{"tags":["contributor"]}},{"cell_type":"markdown","source":"### Contributor2FirstName  Contributor2LastName [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/ORCID_ID_IF_EXIST) \nInstitution","metadata":{"tags":["contributor"]}},{"cell_type":"markdown","source":"### Demival  Vasques Filho [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-4552-0427) \nCentre for Contemporary and Digital History, University of Luxembourg","metadata":{"tags":["contributor"]}},{"cell_type":"markdown","source":"[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n","metadata":{"tags":["copyright"]}},{"cell_type":"markdown","source":"[![cc-by-nc-nd](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/) \n©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n","metadata":{"tags":["copyright"]}},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(\"./media/placeholder.png\"))","metadata":{"tags":["cover"]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" (optional) This article was orginally published (...)","metadata":{"tags":["disclaimer"]}},{"cell_type":"markdown","source":"FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma","metadata":{"tags":["keywords"]}},{"cell_type":"markdown","source":"This is an abstract (...)","metadata":{"tags":["abstract"]}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"\n[we can make comments like this ]: #  \n\n[do we need a citation for \"Unshell\"? ]: #  \n\nIn December 2021, the European Commission presented the \"Unshell\" directive, designed to combat the misuse of shell entities for tax avoidance within the European Union (\"Unshell\"). This measure was introduced in the context of significant strain on public finances during the COVID-19 pandemic, which saw a substantial increase in public deficits across most European states. However, this directive is part of a broader, long-term effort in global tax governance to curb various forms of tax evasion. A transnational tax governance has been in the making since the interwar period, at the latest (Farquet 2010), but moments of intense development were followed by long periods of inactivity. The Unshell directive is part of a longer sequence that started after the financial crisis that hit the world starting in 2007, on the one hand, and by the scandalization of certain practices by journalist consortia, such as the Panama Papers, LuxLeaks, or Cyprus Confidential, on the other hand.","metadata":{}},{"cell_type":"markdown","source":"The OECD and G20 committed to create an international framework to combat tax avoidance by multinational enterprises, starting from 2013 through the Base Erosion and Profit Shifting (BEPS) Action Plan. Since 2016, the European Union has launched several Anti-Tax Avoidance Packages (ATAD). The Unshell directive is part of a third component of ATAD (Sinnig and Zetzsche 2023). At the time of publishing this article, the Unshell directive is still navigating through the labyrinths of the infamous EU trilogue. Subsequently, the transposition of the directive into each member country will reveal its true implications for the tax chains, with Luxembourg sometimes taking a long time to transpose directives that seem unfavorable to its financial center (Bourbaki 2016). Regardless of the fate of the Unshell directive, it has brought to the fore a tool for tax planning and beneficial ownership avoidance: shell companies.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is a hermeneutic paragraph","metadata":{"slideshow":{"slide_type":""},"tags":["hermeneutics"]}},{"cell_type":"markdown","source":"Editor|1641|1798|1916\n---|---|---|---\nSenan|0.55|0.4|0.3\nHenry|0.71|0.5|0.63","metadata":{"jdh":{"module":"object","object":{"source":["table 1: label table 1"]}},"slideshow":{"slide_type":""},"tags":["table-1"]}},{"cell_type":"code","source":"# Check your Python version\nfrom platform import python_version\npython_version()\n\n#!python -V","metadata":{"slideshow":{"slide_type":""},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma","metadata":{"tags":["keywords"]}},{"cell_type":"code","source":"# pandas package needs to be added to the requirements.txt 's file \nimport pandas as pd\n","metadata":{"slideshow":{"slide_type":""},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"https://raw.githubusercontent.com/lux-org/lux-datasets/master/data/college.csv\")\ndf","metadata":{"slideshow":{"slide_type":""},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Appendix (Code Section)","metadata":{}},{"cell_type":"markdown","source":"In this section, we provide a detailed explanation of the code we developed to generate the tables and information presented in this paper","metadata":{"tags":["disclaimer"]}},{"cell_type":"markdown","source":"## Outline","metadata":{}},{"cell_type":"markdown","source":"In this paper, we processed unstructured data in PDF format (scanned documents) through an ETL (Extract, Transform, Load) workflow. The process involved extracting data using 2D line object detection, transforming it into a structured text format with the help of Large Language Models (LLMs), and finally storing it in a database for further analysis. The entire workflow is depicted in Image1.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Updated path with correct file extension\nimage_path = \"/JDH_PAPER/Image1_code_JDH.jpg\"\n\n# Check if the file exists\nimport os\nif not os.path.exists(image_path):\n    print(f\"File not found: {image_path}\")\nelse:\n    # Display the image\n    display(Image(image_path))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:21:53.288527Z","iopub.execute_input":"2025-03-03T12:21:53.288797Z","iopub.status.idle":"2025-03-03T12:21:53.295058Z","shell.execute_reply.started":"2025-03-03T12:21:53.288765Z","shell.execute_reply":"2025-03-03T12:21:53.293926Z"}},"outputs":[{"name":"stdout","text":"File not found: /JDH_PAPER/Image1_code_JDH.jpg\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Image1:  data processing workflow, showcasing the key steps employed to prepare and analyze the data","metadata":{"tags":["disclaimer"]}},{"cell_type":"markdown","source":"### Installation","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"!python.exe -m pip install --upgrade pip","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytesseract\n!pip install opencv-python\n!pip install pdfplumber","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract step of the ETL workflow\nIn the **Extract** step of the **ETL workflow**, we process unstructured PDF documents to extract text data using a combination of **OCR (Optical Character Recognition)** and advanced image preprocessing techniques. First, each page of the PDF is converted into an image using the **pdfplumber** library. These images are preprocessed by converting them to grayscale and applying binary thresholding to enhance text visibility. Line detection is then performed using **morphological operations** to identify horizontal structures, segmenting the image into manageable regions. The extracted regions are processed using **Tesseract OCR** with multilingual support (configured for languages such as French, German, Luxembourgish, and English) to obtain high-quality text data. The extracted text is refined and organized into blocks based on line and paragraph alignment, ensuring readability and accuracy. This extracted text is then aggregated, structured, and saved to a text file for subsequent transformation and loading steps.\n\n### Explanation of the Extraction Code\n\n- **Library Imports:**\n  - Imports libraries for file operations (`os`), image processing (`cv2`, `PIL`), OCR (`pytesseract`), PDF processing (`pdfplumber`), and data handling (`pandas`, `numpy`).  \n\n- **Tesseract Configuration:**\n  - Sets the path to the **Tesseract OCR executable** for text extraction.\n  - Verifies the Tesseract setup by listing installed languages using `subprocess`.\n\n- **Path Setup:**\n  - Defines input and output file paths where the PDFs and processed text files are stored.\n\n- **`extract_text` Function:**\n  - Converts images to grayscale.\n  - Enhances text visibility using **binary thresholding**.\n  - Configures Tesseract for **multilingual OCR** (French, German, Luxembourgish, English).\n  - Extracts text data from the image using Tesseract and organizes it into sorted text blocks.\n  - Handles text alignment and spacing for proper formatting.\n\n- **PDF Processing Loop:**\n  - Iterates through PDFs in the specified directory.\n  - For each PDF:\n    - Converts each page to an image.\n    - Detects horizontal lines using **morphological operations** for segmentation.\n    - Extracts and processes text from cropped sections between lines.\n\n- **Line Detection:**\n  - Preprocesses images for line detection by:\n    - Converting them to grayscale.\n    - Applying **dilation** and **morphological operations**.\n    - Detecting horizontal lines and extracting their coordinates using **contours**.\n\n- **Cropped Region Processing:**\n  - Segments the page into regions based on detected lines.\n  - Extracts text from each cropped region and formats it into structured output.\n\n- **Text Storage:**\n  - Combines extracted text from all pages into a single text file, ensuring each section is clearly demarcated.\n\n- **Visualization:**\n  - Uses `matplotlib` to display original pages and cropped regions during processing for validation.\n\n- **Output:**\n  - Saves the structured text to a file in the output directory and logs the completion status for each PDF.\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"import os\nimport cv2\nimport pytesseract\nimport pdfplumber\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt  # For displaying images\nfrom pytesseract import Output\nimport pandas as pd\nimport subprocess\n\n# Configure Tesseract executable path for Windows\npytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n\n# List installed languages to verify Tesseract setup\nresult = subprocess.run([pytesseract.pytesseract.tesseract_cmd, '--list-langs'], stdout=subprocess.PIPE, text=True)\nprint(\"Installed Tesseract Languages:\\n\", result.stdout)\n\n# Path where processed files will be saved\npathW = \"\\\\JDH_PAPER\\\\\"  # Adjust path to your environment\n\ndef extract_text(image):\n    \"\"\"\n    Extract text from an image using Tesseract OCR.\n\n    Args:\n        image (numpy.ndarray): Input image from which text will be extracted.\n\n    Returns:\n        str: Extracted text.\n    \"\"\"\n    try:\n        # Convert the image to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    except Exception as e:\n        gray = image\n        print(f\"Error during grayscale conversion: {e}\")\n    \n    try:\n        # Apply binary thresholding to enhance text contrast\n        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n    except Exception as e:\n        print(f\"Error during thresholding: {e}\")\n        thresh = image\n\n    # Configure Tesseract for multilingual OCR\n    custom_config = r'-c preserve_interword_spaces=1 --oem 1 --psm 6 -l fra+deu+ltz+eng'\n    \n    # Extract OCR data from image\n    d = pytesseract.image_to_data(thresh, config=custom_config, output_type=Output.DICT)\n    df = pd.DataFrame(d)\n    df1 = df[(df.conf != '-1') & (df.text != ' ') & (df.text != '')]\n\n    # Sort text blocks vertically\n    sorted_blocks = df1.groupby('block_num').first().sort_values('top').index.tolist()\n    text = ''\n    for block in sorted_blocks:\n        curr = df1[df1['block_num'] == block]\n        sel = curr[curr.text.str.len() > 5]\n        char_w = (sel.width / sel.text.str.len()).mean()\n        prev_par, prev_line, prev_left = 0, 0, 0\n        for ix, ln in curr.iterrows():\n            # Add new line when switching paragraphs or lines\n            if prev_par != ln['par_num']:\n                text += '\\n'\n                prev_par = ln['par_num']\n                prev_line = ln['line_num']\n                prev_left = 0\n            elif prev_line != ln['line_num']:\n                text += '\\n'\n                prev_line = ln['line_num']\n                prev_left = 0\n\n            # Calculate space adjustments for alignment\n            added = 0\n            if ln['left'] / char_w > prev_left + 1:\n                added = int((ln['left']) / char_w) - prev_left\n                text += ' ' * added\n            text += ln['text'] + ' '\n            prev_left += len(ln['text']) + added + 1\n        text += \" \\n\"\n    return text\n\n# Process each year within a specific range\nfor year in range(1961, 1962):\n    path = \"\\\\JDH_PAPER\\\\\"  # Adjust this path accordingly\n    List = os.listdir(path)\n    previous_end_crop = \"\"  # Stores the last crop of a page to combine with the next page if necessary\n    \n    for each in List:\n        if each.endswith(\".pdf\"):\n            print(f\"Processing file: {each}\")\n            pdf_file = os.path.join(path, each)\n            outputFilesPath = os.path.join(pathW, each.replace(\".pdf\", \".txt\"))\n            my_pdf = pdfplumber.open(pdf_file)\n            \n            y_all = {i: [] for i in range(len(my_pdf.pages))}\n            All_Text = \"\"\n            All_Text += \"========================================================== \\n\"\n            All_Text += f\"\\n +++++++++++++++++ \\n File name: {each} Page Number: {1}\\n +++++++++++++++++ \\n\"\n            \n            for i in range(len(my_pdf.pages)):\n                # Convert each page to an image\n                im = my_pdf.pages[i].to_image(resolution=420)\n                print(f\"Processing page {i + 1}\")\n                im.save(\"\\\\JDH_PAPER\\\\temporary.png\", \"PNG\")\n                image = cv2.imread(\"\\\\JDH_PAPER\\\\temporary.png\")\n\n                # Display the original page\n                plt.figure(figsize=(12, 10))\n                plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n                plt.title(f\"Original Page {i}\")\n                plt.axis('off')\n                plt.show()\n\n                # Preprocess for line detection\n                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n                thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 1))\n                dilated = cv2.dilate(thresh, kernel, iterations=2)\n                horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (320, 1))\n                detected_lines = cv2.morphologyEx(dilated, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n\n                # Detect lines and extract their coordinates\n                cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n\n                for c in cnts:\n                    y = c[0][0][1]\n                    if y > 0:\n                        y_all[i].append(y)\n\n                y_all[i] = sorted(list(set(y_all[i])))\n                print(\"Detected line y-coordinates:\", y_all[i])\n\n                start = [0] + y_all[i] + [image.shape[0]]\n\n                # Crop and process each region between detected lines\n                for k in range(len(start) - 1):\n                    cropImage = image[start[k]:start[k + 1], :]\n                    plt.figure(figsize=(12, 10))\n                    plt.imshow(cv2.cvtColor(cropImage, cv2.COLOR_BGR2RGB))\n                    plt.title(f\"Page {i} - Crop {k + 1}\")\n                    plt.axis('off')\n                    plt.show()\n\n                    text = extract_text(cropImage)\n                    print(\"Extracted text:\", text)\n\n                    if k == len(start) - 2 and i != len(my_pdf.pages) - 1:\n                        previous_end_crop = text\n                    else:\n                        All_Text += text\n                        All_Text += \"\\n ========================================================== \\n\"\n                        All_Text += f\"\\n +++++++++++++++++ \\n File name: {each} Page Number: {i + 1}\\n +++++++++++++++++ \\n\"\n                \n                if previous_end_crop:\n                    All_Text += previous_end_crop\n                    previous_end_crop = \"\"\n            \n            # Save extracted text to file\n            with open(outputFilesPath, \"w\") as outputFiles:\n                outputFiles.write(All_Text)\n            print(\"Processing completed for:\", each)\n","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text File Processing and Normalization: A Subsection of the Extraction Phase\n\nThis script processes a text file by detecting its encoding, cleaning and normalizing the text, and writing the processed data to a new file. Below are the key steps explained:\n\n1. **Detect File Encoding:**\n   - The `chardet` library is used to automatically detect the encoding of the input file.\n   - This ensures that the file is read correctly, even if it uses non-standard encodings.\n\n2. **Read Input File:**\n   - The input file is opened using the detected encoding, and its content is read line by line into memory.\n\n3. **Process and Normalize Text:**\n   - The script processes the file section by section, where each section is identified by the delimiter `=================================`.\n   - Within each section:\n     - Excessive whitespace is removed using regular expressions.\n     - Text is normalized using `unicodedata.normalize` to ensure consistent Unicode formatting (NFKC normalization).\n     - Problematic characters are replaced or handled to prevent encoding errors during text processing.\n\n4. **Write to Output File:**\n   - The processed text is written to a new file (`1970-01-23_01_Single_Line.txt`) in UTF-8 encoding.\n   - Each section is formatted as a single line with unnecessary line breaks removed.\n\n5. **Output and Reset:**\n   - The processed text for each section is printed to the console for verification.\n   - After writing the section to the output file, the `text` variable is reset to prepare for the next section.\n\n**Final Output**:\nThe cleaned and normalized text is saved in the output file, `1970-01-23_01_Single_Line.txt`, formatted for further analysis or use.\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"import re\nimport unicodedata\nimport chardet  # Library for detecting file encoding\n\n# Function to detect file encoding\ndef detect_encoding(file_path):\n    with open(file_path, 'rb') as f:\n        result = chardet.detect(f.read())\n        return result['encoding']\n\n# Input and output file paths\ninput_file = \"//JDH_PAPER//1970-01-23_01.txt\"  # Replace with your input file path\noutput_file = \"//JDH_PAPER//1970-01-23_01_Single_Line.txt\"  # Replace with your output file path\n\n# Detect input file encoding\ndetected_encoding = detect_encoding(input_file)\nprint(f\"Detected encoding: {detected_encoding}\")\n\n# Open the input file using detected encoding and read all lines\nwith open(input_file, \"r\", encoding=detected_encoding) as file:\n    lines = file.readlines()\n\n# Open the output file in 'utf-8' encoding for writing\nwith open(output_file, \"w\", encoding=\"utf-8\") as write:\n    text = \"\"\n    for line in lines:\n        if \"=================================\" in line.strip():\n            # Normalize text and remove excessive whitespace\n            cleaned_text = re.sub(r'\\s+', ' ', text.lstrip().rstrip().strip()).strip()\n            if len (cleaned_text)>2 and len(cleaned_text.split(\"+++++++++++++++++\")[-1])>2:\n                normalized_text = unicodedata.normalize('NFKC', cleaned_text)\n    \n                # Ensure problematic characters are explicitly handled\n                normalized_text = normalized_text.encode('utf-8', errors='replace').decode('utf-8')\n    \n                # Print and write normalized text\n                print(normalized_text.replace(\"\\n\", \" \"))\n                print(\"*******************\")\n                normalized_text = normalized_text.replace(\"\\n\", \" \") + \"\\n \"\n                write.write(normalized_text )\n                write.flush()\n            # Reset text for next section\n            text = \"\"\n        else:\n            # Accumulate lines in the current section\n            text = text + line.replace(\"\\n\", \" \")\n\nprint(f\"Processed file written to 1970-01-23_01_Single_Line.txt\")\n","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Transformation phase of the ETL workflow\nIn the Transformation phase of the ETL workflow, raw extracted data is cleaned, structured, and enriched to ensure it is in a usable format for analysis. For this paper, we utilized generative AI models, specifically Large Language Models (LLMs), to transform unstructured text into well-structured and semantically enriched formats, enabling more accurate and meaningful insights.\n\n\n\n\n\n\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"markdown","source":"### Explanation of the Code: Extracting Company Names Using Generative AI (LLM) and Cleaning Techniques\n\nThis code demonstrates a workflow to process a text file, extract valid company names using a combination of regex cleaning and a generative AI model, and save the results to an output file. Below is a breakdown of the key components:\n\n- **Library Imports:**\n  - `transformers`: For using pre-trained models like Mistral for text generation.\n  - `torch`: For managing GPU operations and model handling.\n  - `re`: For text cleaning and pattern matching.\n  - `gc`: For memory cleanup during GPU-intensive tasks.\n\n- **Text Cleaning Functions:**\n  - `clean_start_of_text_number1`: Removes leading junk characters and any numbers at the start of the text, ensuring clean input for further processing.\n  - `clean_start_of_text_number2`: Focuses on removing text preceding numbers with 2 or more digits, handling short and uppercase text cases separately.\n\n- **Model Initialization:**\n  - Loads the **Mistral** model (`microsoft/Orca-2-7b`) and tokenizer, specifying the device (CPU or GPU) for processing.\n  - Uses Hugging Face authentication tokens for model access.\n\n- **Company Name Validation:**\n  - `validate_company_name`: Validates and extracts concise company names from descriptions using a pre-trained generative AI model.\n  - Constructs a structured prompt to guide the model in understanding the company description and identifying abbreviations or alternate forms of the name.\n\n- **Company Name Extraction:**\n  - `extract_company_names`: Generates company names from the input text by leveraging a structured prompt and cleaning the model’s output to ensure accuracy.\n  - Handles various cases (e.g., abbreviations, irrelevant text) by splitting based on specific terms like \"Siège social\" or \"Sitz.\"\n\n- **File Processing Workflow:**\n  - Reads the input file line by line.\n  - Cleans the text using `clean_start_of_text_number1` and prepares a structured prompt for the generative model.\n  - Extracts company names using Mistral and validates them against the input text.\n  - Handles edge cases like short or irrelevant lines by filtering invalid results (e.g., returning \"Wrong\" for non-company descriptions).\n\n- **Memory Optimization:**\n  - Periodically clears the GPU cache and collects garbage using `torch.cuda.empty_cache` and `gc.collect`, ensuring efficient resource usage.\n\n- **Output File Generation:**\n  - Writes the extracted company names alongside the original line to an output file (`extracted_company_names.txt`), separating fields with a delimiter (`&&&****&&&`).\n\n- **Final Output:**\n  - The processed file contains a list of valid company names mapped to their corresponding input lines, saved in a structured format for further analysis.\n\n\n\n\n\n\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch,gc\nimport re\n\nimport re\n\ndef clean_start_of_text_number1(input_text):\n    # Remove any punctuation from the first characters\n    cleaned_text = re.sub(r'^[^a-zA-ZÀ-ÖØ-öø-ÿÄäÖöÜüßÇçÉéÈèÊêËëÀàÂâÎîÏïÔôÛûÙùŸÿÆæŒœ]*', '', input_text)\n\n    # Check if there's a number at the start of the text and remove everything before and including it\n    match = re.search(r'^\\s*\\d+', cleaned_text)\n    if match:\n        # Remove the leading number and any preceding characters\n        cleaned_text = cleaned_text[match.end():].strip()\n    else:\n        # If no number at the start, re-clean remaining leading junk characters\n        cleaned_text = re.sub(r'^[^\\wÀ-ÖØ-öø-ÿÄäÖöÜüßÇçÉéÈèÊêËëÀàÂâÎîÏïÔôÛûÙùŸÿÆæŒœ]+', '', cleaned_text).strip()\n    \n    return cleaned_text\n\ndef clean_start_of_text_number2(input_text):\n    # Find the first occurrence of a number with 2 or more digits and remove all text preceding it\n    if len(input_text)<50:\n        end1=len(input_text)\n    else:\n        end1 =35\n        if input_text[:end1].isupper()==True:\n            end1=0\n    match = re.search(r'\\b\\d{2,}', input_text[:end1])\n    if match:\n        # Remove everything before and including the first occurrence of the number\n        cleaned_text = input_text[match.end():].strip()\n    else:\n        # If no number with 2 or more digits is found, return the original input\n        cleaned_text = input_text.strip()\n    \n    return cleaned_text\n    \n# Initialize the Mistral model and tokenizer\nmodel_name = \"microsoft/Orca-2-7b\"#\"Open-Orca/Mistral-7B-OpenOrca\"\n# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Device:\", device)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token='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')\nmodel = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token='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').to(device)\n\ndef validate_company_name(company_description, suggested_company_name, model, tokenizer):\n    \"\"\"\n    Validates and extracts the correct company name based on the provided description and suggested name.\n\n    Args:\n        company_description (str): The company description containing possible company names.\n        suggested_company_name (str): The suggested full company name.\n        model (AutoModelForCausalLM): Pretrained language model for causal generation.\n        tokenizer (AutoTokenizer): Tokenizer corresponding to the pretrained model.\n\n    Returns:\n        str: Extracted concise company name.\n    \"\"\"\n    prompt = (\n        \"You are tasked with validating and extracting the company name from a description. \"\n        \"The company name may be written in full or as an abbreviation.\\n\\n\"\n        \"Instructions:\\n\"\n        \"1. Identify the most concise version of the company name within the provided description.\\n\"\n        \"2. If an abbreviation or alternate form of the name is explicitly stated, extract that form.\\n\"\n        \"3. Return only the exact company name as it appears in the text, without any additional explanation or formatting.\\n\\n\"\n        \"Example 1:\\n\"\n        \"Company Description: \\\"e OMNIUM INTERNATIONAL S.A.» en abréviation: « OMINTER ». Siège social: Luxembourg, 19, Boulevard Prince Henri.\\\"\\n\"\n        \"Suggested Company Name: \\\"OMNIUM INTERNATIONAL S.A.\\\"\\n\"\n        \"Output: \\\"OMINTER\\\"\\n\\n\"\n        \"Example 2:\\n\"\n        \"Company Description: \\\"GLOBALTECH INNOVATIONS GmbH, also referred to as 'GLOBTECH'. Headquartered in Berlin.\\\"\\n\"\n        \"Suggested Company Name: \\\"GLOBALTECH INNOVATIONS GmbH\\\"\\n\"\n        \"Output: \\\"GLOBTECH\\\"\\n\\n\"\n        \"Example 3:\\n\"\n        \"Company Description: \\\"Alpha Corp. Official Name: 'Alpha Corporation'.\\\"\\n\"\n        \"Suggested Company Name: \\\"Alpha Corp.\\\"\\n\"\n        \"Output: \\\"Alpha Corporation\\\"\\n\\n\"\n        \"Company Description: ABC Société Anonyme. fabrique de cuivre, \"\n        \"Suggested Company Name: \\\"ABC, Société Anonyme.  fabrique de cuivre,\\\"\\n\"\n        \"Output: \\\"ABC\\\"\\n\\n\"\n        \"Now process the following:\\n\\n\"\n        f\"Company Description: {company_description}\\n\"\n        f\"Suggested Company Name: {suggested_company_name}\\n\"\n        f\"Output:\"\n    )\n    \n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=50, temperature=0.2, do_sample=False)\n    \n    # Decode the response and extract the result\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    extracted_name = generated_text.split(\"Output:\")[-1].strip()\n    return extracted_name\n\n# Function to generate company name predictions\ndef extract_company_names(prompt, model, tokenizer):\n    # Ensure inputs are on the same device as the model\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n    outputs = model.generate(**inputs, max_new_tokens=30, eos_token_id=tokenizer.eos_token_id)\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"\\n\",\" \").replace(prompt.replace(\"\\n\",\" \"),\"\").replace(\"\\n\",\"\")\n    if \"company name:\" in output_text:\n        output_text=output_text.split(\"company name:\")[1]\n    if \"Company name:\" in output_text:\n        output_text=output_text.split(\"Company name:\")[1]\n    if \"company names:\" in output_text:\n        output_text=output_text.split(\"company names:\")[1]\n    if \"Company names:\" in output_text:\n        output_text=output_text.split(\"Company names:\")[1]\n    if \"Siège social\" in output_text:\n        output_text=output_text.split(\"Siège social\")[0]\n    if \"Siege social\" in output_text:\n        output_text=output_text.split(\"Siege social\")[0]\n    if \"Siége social\" in output_text:\n        output_text=output_text.split(\"Siége social\")[0]\n    if \"Hauptsitz\" in output_text:\n        output_text=output_text.split(\"Hauptsitz\")[0]\n    if \"Sitz\" in output_text:\n        output_text=output_text.split(\"Sitz\")[0]\n    if \"sitz\" in output_text:\n        output_text=output_text.split(\"sitz\")[0]\n    if \"S. à r.\" in output_text:\n        output_text=output_text.split(\"S. à r.\")[0]\n    output_text = clean_start_of_text_number2(output_text)\n    print(\"Output Text: \", output_text)\n    return output_text\n\n\n# Read the input file line by line\ninput_file = \"/JDH_PAPER/1970-01-23_01_Single_Line.txt\"  # Replace with your input text file\noutput_file = \"/JDH_PAPER/extracted_company_names.txt\"\n\nwith open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n    for line in infile:\n        line=line.strip().lstrip().rstrip()\n        if len(line)>2:\n            input_text=line.split('+++++++++++++++++')[-1].strip()\n            if len(input_text)<130:\n                end=len(input_text)\n            else:\n                end=130\n            if \"SOMMAIRE\" in input_text[:20] or 'MEMORIAL Journal Officiel me Amtsblatt' in input_text[:end] or 'RECUEIL SPECIAL ue DES SOCIETES ET ASSOCIATIONS' in input_text[:end]:\n                company_names=\"Wrong\"\n            else:\n                input_text=input_text[:end]\n                input_text=clean_start_of_text_number1(input_text)\n                # input_text=clean_start_of_text(input_text)\n    \n                # Prepare the prompt for Mistral with clear instruction\n                prompt = (\n                    f\"In the following text, company names are short and exact phrases from the input that appear at the start of the given text and are often found before terms like 'social:', 'RECTIFICATIF', 'Hauptsitz:' or 'Sitz:'. \"\n                    f\"If the text is too short or it is not part of a company description, answer with 'Wrong'. \"\n                    f\"It is possible for the given input text to begin with irrelevant characters. For example, in 'TT aaa ttt eeee aa oo 45 ALTA S. A. H. Siège social: Luxembourg,' the company name is 'ALTA S. A. H.' \"\n                    f\"Extract the exact company names from the provided text without any additional explanation. The company name must be an exact substring of the input text. You must only provide the exact company name:\\n\\n\"\n                    f\"{input_text.strip()}\\n\\n\"\n                    f\"Company Names:\"\n                )\n    \n    \n                print(\"prompt:\",prompt)\n                print(\"==========\")\n                generated_text = extract_company_names(prompt, model, tokenizer)\n                generated_text= generated_text.rstrip().lstrip().strip()\n                company_names = generated_text\n                del generated_text\n                torch.cuda.empty_cache()\n                gc.collect()\n                result = validate_company_name(input_text.strip(), company_names, model, tokenizer)\n                company_names = result\n                print(\"Final company_names:\",company_names)\n                print(\"***********************************\")\n\n                del result\n                torch.cuda.empty_cache()\n                gc.collect()\n                # Filter valid company names\n            \n            # Write each company name to the output file\n            outfile.write(company_names +\" &&&****&&& \"+line.replace(\"\\n\",\"\")+ \"\\n\")\n\nprint(f\"Extraction completed. Company names are saved in extracted_company_names.txt.\")\n","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation of the Code: Extracting and Validating Individuals' Names Using Generative AI (LLM) and Regex\n\nThis code processes text data to identify and validate person names using a combination of regex patterns and a generative AI model (Mistral). Below is a breakdown of the key components:\n\n#### **Library Imports and Setup:**\n- **Regex Library:** \n  - Uses the `regex` module to support advanced patterns with Unicode property escapes for handling multilingual text.\n- **Transformers and Torch:**\n  - Loads the `Mistral` model (`mistralai/Mistral-7B-Instruct-v0.3`) for generative AI tasks.\n  - Configures the tokenizer and model on a GPU (if available) or CPU.\n- **Token Authentication:**\n  - Utilizes a Hugging Face token for secure model access.\n\n#### **Core Functions:**\n\n1. **`extract_sequences`:**\n   - Identifies various patterns of person names within a text using regex:\n     - Names with initials (e.g., \"A. Dickes\").\n     - Full names, including titles (e.g., \"Monsieur H.J. Sulkers\").\n     - Multi-word capitalized sequences (e.g., \"Jean Dupont\").\n     - Names with prefixes (e.g., \"Jean de la Croix\").\n     - General sequences of capitalized words.\n   - Combines the matches into a filtered list of potential names, avoiding duplicates.\n\n2. **`clean`:**\n   - Normalizes whitespace by replacing multiple spaces with a single space and trims the text.\n\n3. **`clean_name`:**\n   - Processes names to handle edge cases, such as:\n     - Consecutive uppercase letters.\n     - Cultural variations in name formatting.\n\n4. **`is_person_name`:**\n   - Uses the Mistral model to validate if a given string is likely a person’s name.\n   - Constructs a structured prompt to guide the model in determining name validity.\n   - Generates responses with minimal randomness for consistent results.\n\n#### **File Processing Workflow:**\n\n1. **Input File Reading:**\n   - Reads lines from a file containing company names and related data.\n\n2. **Line Splitting:**\n   - Splits each line into components using the delimiter `&&&****&&&`.\n   - Extracts the last part of the line for further processing.\n\n3. **Regex-Based Name Extraction:**\n   - Applies the `extract_sequences` function to identify potential person names.\n\n4. **Name Validation with AI:**\n   - Filters the names identified by regex using the Mistral model to ensure they are valid person names.\n\n5. **Name Cleaning and Formatting:**\n   - Cleans and formats the validated names, preparing them for output.\n\n6. **Output File Writing:**\n   - Writes the processed data to a new file with structured fields:\n     - Company name.\n     - Validated person names.\n     - Raw text from the input line.\n\n#### **Output:**\n- The processed file contains structured information with validated person names extracted from the input data.\n- Results are saved in a new file with `_people.txt` appended to the original filename.\n\n#### **Highlights:**\n- **Generative AI Integration:** \n  - Enhances regex-based extraction by validating names using a language model.\n- **Multilingual Support:**\n  - Handles names in multiple languages, including French, German, Luxembourgish, and more.\n- **Efficient Processing:**\n  - Optimizes memory usage with periodic cache clearing.\n- **Robustness:**\n  - Handles cultural variations, abbreviations, and irrelevant text effectively.\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"import regex as re  # Importing regex module which supports Unicode property escapes\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n# Replace 'your_huggingface_token' with your actual Hugging Face access token\ntoken = 'your_token'\n\n# Load the model and tokenizer with the token for authentication\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, token=token).to(device)\n\ndef extract_sequences(text):\n    \"\"\"\n    Extract sequences of words that begin with a capital letter from the given text, including specific name patterns.\n\n    Args:\n    text (str): The input text to search within.\n\n    Returns:\n    tuple: Lists for \"A. Dickes\" style sequences, multi-word capitalized sequences,\n           names with geographical or familial prefixes, sequences with titles, and general capitalized word sequences.\n    \"\"\"\n    # Remove extra whitespace from the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Existing patterns\n    pattern_abbr = r'\\b[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ]\\.\\s+[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*'\n    \n    pattern_full_name = r'\\b(?:M\\.|Mlle|Mme|Monsieur|Madame|Mademoiselle|Messieurs|Mesdames|Mesdemoiselles|Herr|Frau|Fräulein|Herren|Doktor|Dr\\.|Här|Madamm|Härn|Sr\\.|Sra\\.|Srta\\.|Don|Doña|Sres\\.|Sras\\.|Dres\\.|Veuve|Witwe|Wittfra|Viuda\\s+)?(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s?)+(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ]\\.\\s)?[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*'\n    \n    pattern_multi_word = r'\\b(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s+){2,}'\n    \n    pattern_prefix = r'\\b[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s+(?:de|van|von|de la|de los|de las)\\s+[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*'\n    \n    pattern_general = r'\\b(?:[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]*\\s*){2,}'\n    \n    # New pattern for names with initials and surname, such as \"Monsieur H.J. Sulkers\"\n    pattern_initials_with_title = r'\\b(?:M\\.|Monsieur|Madame|Herr|Frau|Dr\\.|Doktor|Här|Madamm)\\s+[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ](?:\\.[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ])?\\.\\s*[A-ZÄÖÜßÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝŸČĆŠŽŒ][a-zäöüßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýÿčćšžœ\\'\\-]+'\n\n    \n    # Find all matches for \"A. Dickes\" style sequences\n    matches_abbr = re.findall(pattern_abbr, text)\n\n    # Find all matches for full names including those with initials\n    matches_full_name = re.findall(pattern_full_name, text)\n\n    # Find all matches for multi-word capitalized sequences\n    matches_multi_word = re.findall(pattern_multi_word, text)\n\n    # Find all matches for names with prefixes like \"de\", \"van\", \"von\", \"de la\", etc.\n    matches_prefix = re.findall(pattern_prefix, text)\n\n    # Find all matches for general capitalized word sequences\n    matches_general = re.findall(pattern_general, text)\n\n    # Find all matches for pattern_initials_with_title sequences\n    matches_pattern_initials_with_title = re.findall(pattern_initials_with_title, text)\n    \n    # Combine all matches into one list for multi-word capitalized sequences\n    combined_matches_multi_word = set(matches_full_name + matches_multi_word + matches_prefix + matches_general+matches_pattern_initials_with_title)\n    \n    cr = []\n    for each_item_c in combined_matches_multi_word:\n        if len(each_item_c.split(\" \")) > 1:\n            cr.append(each_item_c)\n\n    # Remove any matches that are already included in matches_abbr to avoid duplication\n    combined_matches_multi_word = [match.strip() for match in cr if match.strip() not in matches_abbr]\n\n    return matches_abbr, combined_matches_multi_word, matches_prefix, matches_full_name, matches_general\n\n\ndef clean(text_in):\n    return re.sub(r'\\s+', ' ', text_in).strip()\n\ndef clean_name(name):\n    # Split the name into parts to handle the first word separately\n    name = name.replace(\"MM \", \"\")\n    parts = name.split()\n\n    # Process only the first part to clean names\n    first_part = parts[0]\n\n    # Check if the first part has multiple consecutive uppercase letters and at least one lowercase letter\n    index = 0\n    for char in first_part:\n        if char.isupper():\n            index += 1\n        else:\n            break\n\n    # If the first word has more than one consecutive uppercase letter and at least one lowercase, remove all but the last\n    if index > 1 and any(char.islower() for char in first_part):\n        first_part = first_part[index-1:]\n    \n    # Replace the first part in the list of parts\n    parts[0] = first_part\n\n    # Reassemble the cleaned parts into a single string\n    cleaned_name = ' '.join(parts).strip()\n\n    return cleaned_name\n\n# Function to use the model to check if a text is a person's name\ndef is_person_name(name):\n    prompt = (f\"Determine if the following text is a person's name. \"\n              f\"Consider potential typos or cultural variations in names: '{name}'. \"\n              f\"Additionally, phrases such as 'Recueil Spécial' or 'Le Receveur' or 'Signatures Enregistré' are not names for people. \"\n              f\"If the text is highly likely a person's name, answer 'yes'. Otherwise, answer 'no'.\")\n    \n    # Tokenize and move to correct device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate response on the same device\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=30,\n        temperature=0.2  # Lower randomness for consistent answers\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"'yes'. Otherwise, answer 'no'.\",\"\")\n    \n    if \"Answer:\" in response:\n        answer = response.split(\"Answer:\")[-1].strip().lower()\n    else:\n        answer = response.strip().lower()\n    \n    return 'yes' in answer\n\n\n# Read the file line by line with Latin-1 encoding\n\npathfiles = [\n    '/JDH_PAPER/extracted_company_names.txt'\n]\n\nfor file_path in pathfiles:\n    \n    write = open(file_path.replace(\".txt\", \"_people.txt\"), \"w\", encoding='utf-8')\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            # Step 1: Strip the line and split using the specified delimiter \"&&&***&&&\"\n            parts = line.strip().replace(\" MM\",\" MM \").split('&&&****&&&')\n            company = clean(parts[0])\n            print(\"company: \", company)\n            # Step 2: Extract the last item from the split parts\n            last_part = clean(parts[-1]).strip()\n            if \"wrong\" not in company.lower():\n                # Step 3: Call the function to extract sequences\n                list_abbr, list_multi_word, list_prefix, list_full_name, list_general = extract_sequences(last_part)\n    \n                # Step 4: Print the results for this line\n                print(\"list_abbr: \", list_abbr)\n                abb = \"\"\n                for each_i in range(0, len(list_abbr) - 1):\n                    list_abbr[each_i] = clean(list_abbr[each_i])\n                    abb = abb + list_abbr[each_i] + \" *_* \"\n                if len(list_abbr) > 0:\n                    abb = abb + list_abbr[-1]\n                print(\"abb: \", abb)\n    \n                print(\"list_multi_word: \", list_multi_word)\n                filtered_names = [name for name in list_multi_word if is_person_name(name.replace(\"MM \", \"\"))]\n                print(filtered_names)\n                filtered_names_1=[]\n                for Fil in filtered_names:\n                    if Fil not in  filtered_names_1:\n                        filtered_names_1.append(Fil)\n                filtered_names=filtered_names_1\n                print(\"filtered_names: \", filtered_names)\n    \n                fn = \"\"\n                for each_i in range(0, len(filtered_names) - 1):\n                    filtered_names[each_i] = clean(filtered_names[each_i])\n                    filtered_names[each_i] = clean_name(filtered_names[each_i])\n                    fn = fn + filtered_names[each_i] + \" *_* \"\n                if len(filtered_names) > 0:\n                    fn = fn + filtered_names[-1]\n            else:\n                fn=\"\"\n                abb=\"\"\n            print(\"fn: \", fn)\n            print(\"***************************************************************************\")\n\n            fn = clean(fn)\n            abb = clean(abb)\n\n            print(\"******************\")\n            newline = company.replace('\"',\"\") + \" &&&***&&& \" + company.replace('\"',\"\") + \" &&&***&&& \" + fn + \" &&&***&&& \" + abb + \" &&&***&&& \" +\\\n            last_part + \"\\n\"\n            write.write(newline)\n            write.flush()\n\n            print(\"-\" * 40)  # Separator for output clarity\n    write.close()\nprint(\"done\")","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation of the Code for Fine-Tuning a GPT-2 Model to Identify Addresses\n\nThis code implements a complete pipeline for fine-tuning a GPT-2 model with additional capabilities like LoRA (Low-Rank Adaptation) for parameter-efficient training, response generation, and selective model merging. Below is a detailed explanation of each part:\n\n\n\n#### **Class Initialization (`FineTune_GPT2`)**\n\n1. **Initialization of Class Attributes:**\n   - `dataset_path`: Specifies the path to the dataset used for training.\n   - `cache_dir`: Directory to store the cached models and tokenizers.\n   - `token`: Hugging Face API token for accessing gated models or datasets.\n\n2. **Model Configuration:**\n   - Sets up a custom configuration for GPT-2 using `GPT2Config` with parameters like:\n     - Hidden size, number of attention heads, and hidden layers.\n     - Vocab size and activation function (`gelu_new`).\n   - Loads the tokenizer and the GPT-2 model from Hugging Face, using the custom configuration.\n\n3. **Dataset Loading:**\n   - Loads the dataset from the specified `dataset_path` using the `datasets` library.\n   - Assumes the dataset is in JSON format and splits it into training data.\n\n\n#### **Model Training (`train_model`)**\n\n1. **LoRA Configuration:**\n   - Configures LoRA (`LoraConfig`) for fine-tuning specific layers of the model (e.g., attention layers, projection layers).\n   - Enables efficient fine-tuning by updating only a subset of parameters.\n\n2. **Training Arguments:**\n   - Specifies training parameters like:\n     - `num_train_epochs`: Number of training epochs.\n     - `per_device_train_batch_size`: Batch size for training.\n     - `logging_steps`: Interval for logging training progress.\n     - `save_steps`: Interval for saving model checkpoints.\n   - Logs data to TensorBoard for monitoring.\n\n3. **Trainer Setup and Training:**\n   - Uses `SFTTrainer` (Supervised Fine-Tuning Trainer) for training the model.\n   - Passes the fine-tuning configuration, dataset, tokenizer, and training arguments.\n   - Trains the model and saves it to the specified output directory.\n\n\n#### **Response Generation (`generate_response`)**\n\n1. **Prompt Construction:**\n   - Constructs a prompt containing a question for the model to answer.\n   - Ensures the prompt guides the model to generate a concise, single response.\n\n2. **Tokenization and Generation:**\n   - Tokenizes the prompt and generates a response using the fine-tuned model.\n   - Configures generation parameters:\n     - `max_new_tokens`: Maximum tokens to generate.\n     - `temperature`: Controls randomness in output (lower values produce deterministic responses).\n\n3. **Output Cleaning:**\n   - Decodes the generated tokens and removes unnecessary parts of the output (e.g., repeated prompt text).\n   - Returns the cleaned response.\n\n\n#### **Resource Cleanup (`clean_up`)**\n\n1. **Memory Management:**\n   - Deletes the tokenizer and model to free up memory.\n   - Uses garbage collection (`gc.collect`) and clears GPU cache (`torch.cuda.empty_cache`).\n\n\n\n#### **Selective Model Merging (`selective_merge`)**\n\n1. **Load Base and Fine-Tuned Models:**\n   - Loads the pre-trained base GPT-2 model and the fine-tuned model from specified paths.\n\n2. **Merge State Dicts:**\n   - Compares the state dictionaries of both models and updates the base model's parameters with those of the fine-tuned model, ensuring size compatibility.\n\n3. **Save Merged Model:**\n   - Saves the updated model, tokenizer, and configuration to the specified output directory.\n\n\n#### **Main Program Workflow**\n\n1. **Instantiate the Class:**\n   - Initializes the `FineTune_GPT2` class with the dataset path, cache directory, and Hugging Face token.\n\n2. **Train the Model:**\n   - Calls `train_model` to fine-tune the GPT-2 model on the dataset.\n   - Saves the trained model to the specified output directory.\n\n3. **Merge Models (Optional):**\n   - Calls `selective_merge` to combine the fine-tuned model with the base model, retaining compatibility.\n\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"markdown","source":"#### installation","metadata":{"tags":["hidden"]}},{"cell_type":"code","source":"!pip install peft\n!pip install trl\n!pip install tensorboard\n!pip install tensorboardX","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    GPT2Config  # Import the GPT-2 configuration class\n)\nfrom peft import LoraConfig, PeftModel\nfrom random import randrange\nfrom trl import SFTTrainer  # Correct import for the SFTTrainer\n\nclass FineTune_GPT2:\n    def __init__(self, dataset_path, cache_dir, token):\n        \"\"\"\n        Initialize the FineTune_GPT2 class.\n\n        Args:\n            dataset_path (str): Path to the dataset.\n            cache_dir (str): Directory to cache the models and tokenizers.\n            token (str): Hugging Face API token for accessing gated repositories.\n\n        How to obtain the Hugging Face API token:\n        1. Go to https://huggingface.co/settings/tokens\n        2. Log in or sign up if you don't have an account.\n        3. Create a new token with the required permissions (read access is sufficient).\n        4. Copy the token and provide it when initializing this class.\n        \"\"\"\n        self.dataset_path = dataset_path\n        self.cache_dir = cache_dir\n        self.token = token\n\n        # Embedded configuration\n        config_dict = {\n            \"architectures\": [\"GPT2LMHeadModel\"],\n            \"bos_token_id\": 50256,\n            \"eos_token_id\": 50256,\n            \"hidden_act\": \"gelu_new\",\n            \"hidden_size\": 768,\n            \"initializer_range\": 0.02,\n            \"intermediate_size\": None,\n            \"layer_norm_eps\": 1e-05,\n            \"model_type\": \"gpt2\",\n            \"num_attention_heads\": 12,\n            \"num_hidden_layers\": 12,\n            \"vocab_size\": 50257\n        }\n        self.config = GPT2Config.from_dict(config_dict)  # Use the correct configuration class\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"gpt2\", \n            cache_dir=self.cache_dir, \n            use_auth_token=self.token  # Use the correct parameter\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token  # Set padding token to eos token\n        self.tokenizer.padding_side = 'right'  # Ensure padding is on the right\n        self.trained_model = AutoModelForCausalLM.from_pretrained(\n            \"gpt2\", \n            config=self.config,\n            cache_dir=self.cache_dir,\n            use_auth_token=self.token  # Use the correct parameter\n        ).to(\"cuda\")\n        self.dataset = load_dataset('json', data_files=self.dataset_path, split='train')\n\n    def train_model(self, output_dir, num_train_epochs=3, per_device_train_batch_size=2, per_device_eval_batch_size=1, max_seq_length=None):\n        lora_config = LoraConfig(\n            lora_alpha=16,\n            lora_dropout=0.1,\n            r=64,\n            target_modules=[\n                \"attn.c_attn\",\n                \"attn.c_proj\",\n                \"mlp.c_fc\",\n                \"mlp.c_proj\",\n            ],\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=num_train_epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=per_device_eval_batch_size,\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=10000,\n            save_steps=1000,\n            warmup_ratio=0.03,\n            report_to=\"tensorboard\"\n        )\n\n        trainer = SFTTrainer(\n            model=self.trained_model,\n            train_dataset=self.dataset,\n            peft_config=lora_config,\n            dataset_text_field=\"text\",  # Assuming 'text' is the field name containing the text data\n            max_seq_length=max_seq_length,  # Pass None or specify a maximum sequence length\n            tokenizer=self.tokenizer,\n            args=training_args\n        )\n\n        trainer.train()\n        trainer.model.save_pretrained(output_dir)\n        print(\"The new model is available in \" + output_dir)\n        self.trained_model = trainer.model\n\n    def generate_response(self, question, max_new_tokens=500, temperature=0.1):\n        prompt = f\"\"\"You will be provided with a question. You must provide only a single answer. You must not provide additional questions and answers.\n        Question:\n        {question}\n        \"\"\"\n        model_input = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        with torch.no_grad():\n            generated_code = self.trained_model.generate(**model_input, max_new_tokens=max_new_tokens, pad_token_id=self.tokenizer.eos_token_id, temperature=temperature)\n            generated_code = self.tokenizer.decode(generated_code[0], skip_special_tokens=True)\n            response = generated_code.split(\"You will be provided with a question\")[1]\n            if len(response) < 10:\n                return generated_code\n        return response\n\n    def clean_up(self):\n        del self.tokenizer\n        del self.trained_model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def selective_merge(self, base_model_path, fine_tuned_model_path, output_dir):\n        base_model = AutoModelForCausalLM.from_pretrained(base_model_path, cache_dir=self.cache_dir, use_auth_token=self.token).to(\"cuda\")\n        fine_tuned_model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path, cache_dir=self.cache_dir, use_auth_token=self.token).to(\"cuda\")\n\n        # Extract state dicts\n        base_state_dict = base_model.state_dict()\n        ft_state_dict = fine_tuned_model.state_dict()\n\n        # Filter out keys: only update base model with keys that exist in its state dict and have the same size\n        for key in ft_state_dict:\n            if key in base_state_dict and ft_state_dict[key].size() == base_state_dict[key].size():\n                base_state_dict[key] = ft_state_dict[key]\n\n        # Load the filtered state dict back into the base model\n        base_model.load_state_dict(base_state_dict, strict=False)\n\n        # Save the merged model\n        base_model.save_pretrained(output_dir)\n\n        # Save tokenizer and configuration as well\n        self.tokenizer.save_pretrained(output_dir)\n        self.config.save_pretrained(output_dir)\n\n        print(f\"Merged model, tokenizer, and config are saved in {output_dir}\")\n\n        return base_model\n\n\n\n# Instantiate and use the FineTune_GPT2 class\ndataset_path = \"/JDH_PAPER/training.jsonl\"\nhf_token = \".............\" # your token\ncache_dir = \"/JDH_PAPER\"\noutput_dir = \"/JDH_PAPER/Models/\"\nmerged_model_output_dir = \"/JDH_PAPER/Merged\"\n\n# Create an instance of FineTune_GPT2 and start training\nfine_tuner = FineTune_GPT2(dataset_path, cache_dir, token=hf_token)\n\n\n# Train the model and save it to the output directory\nfine_tuner.train_model(output_dir=output_dir, num_train_epochs=25, per_device_train_batch_size=2, per_device_eval_batch_size=1)\n\n# Optionally, merge the fine-tuned model with the base model\nfine_tuner.selective_merge(base_model_path=output_dir, fine_tuned_model_path=output_dir, output_dir=merged_model_output_dir)\n\n","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Description of Using the Fine-Tuned GPT-2 Model for Address Extraction\n\nThis code demonstrates how to use a fine-tuned GPT-2 model to extract addresses from text. It integrates preprocessing, inference using the fine-tuned model, and post-processing steps. Below is an explanation of each part:\n\n\n#### **Class Initialization (`FineTune_GPT2_use`)**\n\n1. **Initialization of Attributes:**\n   - `cache_dir`: Directory for caching models and tokenizer files.\n   - `token`: Hugging Face API token for accessing the fine-tuned model.\n   - `model_path`: Path to the directory containing the fine-tuned model.\n\n2. **Model and Tokenizer Loading:**\n   - Loads the tokenizer and model from the specified `model_path`.\n   - Configures the model to use `torch.float16` for memory efficiency on GPUs.\n\n3. **Padding Configuration:**\n   - Sets the padding side of the tokenizer to `right` for consistent text alignment during tokenization.\n\n\n\n####  **Generating Responses (`generate_response`)**\n\n1. **Prompt Construction:**\n   - Constructs a question prompt to guide the model in extracting an address from the provided text.\n\n2. **Tokenization:**\n   - Converts the prompt into tokenized input compatible with the model's architecture.\n   - Ensures inputs are truncated to fit within the model's maximum input size.\n\n3. **Text Generation:**\n   - Generates a response from the fine-tuned GPT-2 model with specified parameters:\n     - `max_new_tokens`: Maximum length of the generated response.\n     - `temperature`: Controls randomness in the model's output (lower values result in more deterministic outputs).\n\n4. **Output Decoding:**\n   - Decodes the generated tokens into human-readable text.\n   - Removes special tokens and extraneous parts of the output.\n\n#### **Processing Input Files and Extracting Addresses**\n\n1. **Input File Handling:**\n   - Reads lines from an input file containing company information.\n\n2. **Prompt Preparation:**\n   - Splits each line into components using the delimiter `&&&***&&&`.\n   - Extracts relevant text from the last component to construct a question prompt.\n   - Limits the length of the input text to avoid exceeding the model's maximum input size.\n\n3. **Address Extraction:**\n   - Passes the prompt to the `generate_response` method to extract the address.\n   - Cleans the response by:\n     - Removing unnecessary whitespace.\n     - Extracting the address portion using regex patterns.\n     - Handling cases where no valid address is found by returning `\"No Addr\"`.\n\n#### **Post-Processing and Writing Results**\n\n1. **Refinement:**\n   - Further cleans the extracted address using regex to improve readability.\n\n2. **Writing to Output File:**\n   - Combines the extracted address with the original data and writes it to a new output file.\n   - Ensures each entry in the output file is formatted consistently.\n\n#### **Error Handling**\n\n1. **Model Reinitialization:**\n   - Handles potential runtime issues by reinitializing the model in case of failures during address extraction.\n\n2. **Graceful Fallback:**\n   - If address extraction fails or produces invalid output, a placeholder (`\"No Addr\"`) is used.\n\n**Cleanup (`clean_up`)**\n\n1. **Memory Management:**\n   - Deletes the model and tokenizer to free up GPU memory.\n   - Uses garbage collection (`gc.collect()`) and clears GPU cache (`torch.cuda.empty_cache`).\n\n#### **Main Workflow**\n\n1. **Initialize the Model:**\n   - Loads the fine-tuned GPT-2 model from the specified directory.\n\n2. **Process Input Files:**\n   - Reads company information from an input file and extracts addresses line by line.\n\n3. **Generate Prompts and Extract Addresses:**\n   - Constructs prompts and uses the fine-tuned model to generate responses containing addresses.\n\n4. **Refine and Save Results:**\n   - Cleans and validates the extracted addresses.\n   - Writes the processed results to an output file for further use.\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport gc\nimport torch\nimport os,re\n\n\nclass FineTune_GPT2_use:\n    def __init__(self, cache_dir=None, token=None, model_path=None):\n        \"\"\"\n        Initialize the class with cache_dir, token, and model_path for the GPT2 model.\n        \"\"\"\n        self.cache_dir = cache_dir\n        self.token = token\n\n        # Load the tokenizer and model from the specified path (fine-tuned model folder)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=self.cache_dir, token=self.token)\n\n        # Load the model on CPU for debugging, or on CUDA with float16 if memory is an issue\n        self.trained_model = AutoModelForCausalLM.from_pretrained(\n            model_path, \n            cache_dir=self.cache_dir, \n            token=self.token, \n            torch_dtype=torch.float16  # Try using float16 to reduce memory\n        ).to(\"cuda\")\n\n        # Set padding token for the tokenizer\n        # self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = 'right'\n\n    def generate_response(self, question, max_new_tokens=500, temperature=0.1):\n        \"\"\"\n        Generate a response to the given question using the fine-tuned GPT2 model.\n\n        Args:\n            question (str): The question to ask the model.\n            max_new_tokens (int): Maximum number of tokens to generate.\n            temperature (float): The temperature for generation.\n\n        Returns:\n            str: The generated response.\n        \"\"\"\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        # Tokenize the input and print for debugging\n        inputs = self.tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=1024)\n        inputs = inputs.to(device)  # Move inputs to the same device as the model\n\n        # Generate text with no gradient computation\n        with torch.no_grad():\n            generated_output = self.trained_model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                pad_token_id=self.tokenizer.eos_token_id, \n                temperature=temperature\n            )\n            # Decode the generated text and remove special tokens\n            generated_text = self.tokenizer.decode(generated_output[0], skip_special_tokens=True)\n\n        return generated_text\n\n    def clean_up(self):\n        \"\"\"\n        Clean up the resources by deleting the model and tokenizer, and clearing the GPU cache.\n        \"\"\"\n        del self.tokenizer\n        del self.trained_model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n# Run the model on CPU or GPU with debug options to find the problem\nif __name__ == \"__main__\":\n    gpt2_model_path = \"/JDH_PAPER/Merged/\"\n    fine_tuner1 = FineTune_GPT2_use(cache_dir='/JDH_PAPER', token='use_your_token', model_path=gpt2_model_path)\n    pathfiles = [\n        '/JDH_PAPER/extracted_company_names_people.txt'\n    ]\n#     question = \"Where is the address of the company in the following text?\"\n#     response = fine_tuner.generate_response(question)\n#     print(\"response:\",response)\n    for each_fine in pathfiles:\n        dataset_path = each_fine.replace(\"_people.txt\", \"_people_Addr.txt\")\n        write = open(dataset_path, \"w\", encoding='utf-8')\n        read_file = each_fine\n\n        read = open(read_file, 'r', encoding='utf-8')\n        for line in read:\n            # Process each line and extract relevant information\n            L = line.split(\" &&&***&&& \")\n            L[-1]=L[-1].rstrip().lstrip().strip()\n            CopyL=L[-1]\n            L[-1] = L[-1].split(\"+++++++++++++++++\")[-1].replace(\" : \", \": \")\n\n            # Step 1: Use GPT-2 fine-tuned model to find address in text\n            te=\"\"\n            size_L=50\n            if len(L[-1].split(\" \"))<50:\n                size_L=len(L[-1].split(\" \"))\n            for e in range(0,size_L):\n                te=te+L[-1].split(\" \")[e]+\" \"\n            question=\"Where is the address of the company in the following text? \"+te \n            print(\"question: \",question)\n            try:\n                address=fine_tuner1.generate_response(question)\n            except:\n                del fine_tuner1\n                fine_tuner1 = FineTune_GPT2_use(cache_dir='/JDH_PAPER/', token='your_token', model_path=gpt2_model_path)\n\n            address = re.sub(r'\\s+', ' ', address)  # Clean the response\n            \n            try:\n                # Extract address, handle missing or incorrect data gracefully\n                address = re.sub(r'\\s+', ' ', address).split(\"Answer:\")[1].replace(\"The Address is :\", \"\").replace(\"</s>\", \"\").replace(\"</s\", \"\").replace(\"/s>\", \"\")\n            except:\n                print(\"To be checked: \", address)\n                address = \"No Addr\"\n\n            address = re.sub(r'\\s+', ' ', address)\n            address=re.sub(r\"(?<!^)(?=[A-Z])\", \" \", address)\n            address = re.sub(r'\\s+', ' ', address)  # Clean the response\n            if len(address)>120:\n                address=\"No Addr\"\n\n            refined_address = address  # Refinement step could be added here\n            print(\"++++++++++\")\n            print(\"refined address:\", refined_address)\n            print(\"++++++++++\")\n            print(\"=====================================================================\")\n\n            # Write results back to the file\n            newline = L[0]+\" &&&***&&& \"+L[1]+\" &&&***&&& \"\\\n            +L[2]+\" &&&***&&& \"+L[3]+\" &&&***&&& \"+refined_address+\" &&&***&&& \"+CopyL.replace('\\n', '')+\"\\n\"\n\n            write.write(newline)\n            write.flush()\n\n        write.close()\n        read.close()\n\n        # Clean up resources after processing\n        fine_tuner1.clean_up()\n    ","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanation of the Code for Extracting Country Names from Text\n\nThis code is designed to identify and extract country names from text data. It uses a predefined dictionary of countries in multiple languages and processes input lines from a text file to match and extract country names. Below is a step-by-step explanation:\n\n\n#### **Key Components**\n\n1. **Countries Dictionary:**\n   - A dictionary (`countries`) defines country names in English, French, German, and Luxembourgish.\n   - Each country is mapped to a list of its possible representations in different languages.\n\n2. **Input and Output Files:**\n   - The input file (`extracted_company_names_people_Addr.txt`) contains company information, potentially including country references.\n   - The output file (`final_a.txt`) will store the processed lines with extracted country names appended.\n\n\n#### **Processing Workflow**\n\n1. **Read Input Line by Line:**\n   - The input file is read line by line, splitting each line using the delimiter `&&&***&&&` to separate components like company name, address, and description.\n\n2. **Text Preprocessing:**\n   - Each component of the line is cleaned by:\n     - Removing excessive whitespace using `re.sub(r'\\s+', ' ', ...)`.\n     - Stripping leading and trailing whitespace.\n\n3. **Country Name Matching:**\n   - For each line:\n     - Iterates through all keys (country names) in the `countries` dictionary.\n     - Creates a list of possible country name variations (from the dictionary values).\n     - Uses a regex pattern to search for matches in the last component of the line (description or address):\n       - Matches the country name (`t_e`) followed by a non-alphabetic character or the end of the string.\n       - Example pattern: `\\b{country_name}\\b(?=\\s|[^a-zA-ZÀ-ÿ]|$)`.\n\n4. **Special Case for \"Jersey\" vs \"New Jersey\":**\n   - Handles the ambiguity between \"Jersey\" and \"New Jersey\" by counting occurrences:\n     - Ensures \"Jersey\" is not extracted if \"New Jersey\" is more frequent.\n\n5. **Unique Country Names:**\n   - Deduplicates the list of extracted country names using `list(set(names_c))`.\n\n6. **Formatting the Output:**\n   - Combines extracted country names into a formatted string (`*_*-separated`) for easier parsing in the output file.\n\n7. **Write Results to Output File:**\n   - Constructs a new line with the original components and the extracted country names appended.\n   - Writes the processed line to the output file.\n\n#### **Output Details**\n\n1. **Example Output Line:**\n   - Each output line retains the original components and appends the extracted country names:\n     ```\n     Company Name &&&***&&& Description &&&***&&& Extracted Country Names &&&***&&& Address\n     ```\n\n2. **Console Logging:**\n   - For each line, prints:\n     - The company name.\n     - The description or relevant text for country matching.\n     - The extracted country names for verification.\n\n\n#### **Key Features**\n\n- **Multi-Language Support:**\n  - The `countries` dictionary accounts for country names in English, French, German, and Luxembourgish.\n  \n- **Robust Regex Matching:**\n  - Ensures country names are matched only as standalone words, avoiding partial matches.\n\n- **Special Case Handling:**\n  - Differentiates between similar names like \"Jersey\" and \"New Jersey.\"\n\n- **Formatted Output:**\n  - Extracted country names are appended in a structured format (`*_*-separated`) for clarity and ease of downstream processing.\n\n\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"countries = {\n    \"Andorra\": [\"Andorre\", \"Andorra\", \"Andorra\"],\n    \"Anguilla\": [\"Anguilla\", \"Anguilla\", \"Anguilla\"],\n    \"Antigua and Barbuda\": [\"Antigua-et-Barbuda\", \"Antigua und Barbuda\", \"Antigua an Barbuda\"],\n    \"Aruba\": [\"Aruba\", \"Aruba\", \"Aruba\"],\n    \"Bahamas\": [\"Bahamas\", \"Bahamas\", \"Bahamas\"],\n    \"Bahrain\": [\"Bahreïn\", \"Bahrain\", \"Bahrain\"],\n    \"Barbados\": [\"Barbade\", \"Barbados\", \"Barbados\"],\n    \"Belize\": [\"Belize\", \"Belize\", \"Belize\"],\n    \"Bermuda\": [\"Bermudes\", \"Bermuda\", \"Bermuda\"],\n    \"British Virgin Islands\": [\"Îles Vierges britanniques\", \"Britische Jungferninseln\", \"Britesch Jofferinselen\"],\n    \"Cayman Islands\": [\"Îles Caïmans\", \"Kaimaninseln\", \"Kaimaninselen\"],\n    \"Cook Islands\": [\"Îles Cook\", \"Cookinseln\", \"Cookinselen\"],\n    \"Costa Rica\": [\"Costa Rica\", \"Costa Rica\", \"Costa Rica\"],\n    \"Cyprus\": [\"Chypre\", \"Zypern\", \"Zypern\"],\n    \"Djibouti\": [\"Djibouti\", \"Dschibuti\", \"Dschibuti\"],\n    \"Gibraltar\": [\"Gibraltar\", \"Gibraltar\", \"Gibraltar\"],\n    \"Grenada\": [\"Grenade\", \"Grenada\", \"Grenada\"],\n    \"Guernsey\": [\"Guernesey\", \"Guernsey\", \"Guernsey\"],\n    \"Hong Kong\": [\"Hong Kong\", \"Hongkong\", \"Hong Kong\"],\n    \"Ireland\": [\"Irlande\", \"Irland\", \"Irland\"],\n    \"Isle of Man\": [\"Île de Man\", \"Isle of Man\", \"Isle of Man\"],\n    \"Jersey\": [\"Jersey\", \"Jersey\", \"Jersey\"],\n    \"Jordan\": [\"Jordanie\", \"Jordanien\", \"Jordanien\"],\n    \"Lebanon\": [\"Liban\", \"Libanon\", \"Libanon\"],\n    \"Liberia\": [\"Libéria\", \"Liberia\", \"Liberia\"],\n    \"Liechtenstein\": [\"Liechtenstein\", \"Liechtenstein\", \"Liechtenstein\"],\n    \"Luxembourg\": [\"Luxembourg\", \"Luxemburg\", \"Lëtzebuerg\"],\n    \"Macao\": [\"Macao\", \"Macao\", \"Macao\"],\n    \"Maldives\": [\"Maldives\", \"Malediven\", \"Malediven\"],\n    \"Malta\": [\"Malte\", \"Malta\", \"Malta\"],\n    \"Marshall Islands\": [\"Îles Marshall\", \"Marshallinseln\", \"Marshallinselen\"],\n    \"Mauritius\": [\"Île Maurice\", \"Mauritius\", \"Mauritius\"],\n    \"Micronesia\": [\"Micronésie\", \"Mikronesien\", \"Mikronesien\"],\n    \"Monaco\": [\"Monaco\", \"Monaco\", \"Monaco\"],\n    \"Montserrat\": [\"Montserrat\", \"Montserrat\", \"Montserrat\"],\n    \"Nauru\": [\"Nauru\", \"Nauru\", \"Nauru\"],\n    \"Netherlands\": [\"Pays-Bas\", \"Niederlande\", \"Holland\"],\n    \"Niue\": [\"Niue\", \"Niue\", \"Niue\"],\n    \"Panama\": [\"Panama\", \"Panama\", \"Panama\"],\n    \"Samoa\": [\"Samoa\", \"Samoa\", \"Samoa\"],\n    \"San Marino\": [\"Saint-Marin\", \"San Marino\", \"San Marino\"],\n    \"Seychelles\": [\"Seychelles\", \"Seychellen\", \"Seychellen\"],\n    \"Singapore\": [\"Singapour\", \"Singapur\", \"Singapur\"],\n    \"Saint Kitts and Nevis\": [\"Saint-Christophe-et-Niévès\", \"St. Kitts und Nevis\", \"St. Kitts an Nevis\"],\n    \"Saint Lucia\": [\"Sainte-Lucie\", \"St. Lucia\", \"St. Lucia\"],\n    \"Saint Martin\": [\"Saint-Martin\", \"Saint Martin\", \"Saint Martin\"],\n    \"Saint Vincent and the Grenadines\": [\"Saint-Vincent-et-les-Grenadines\", \"St. Vincent und die Grenadinen\", \"St. Vincent an d'Grenadinnen\"],\n    \"Switzerland\": [\"Suisse\", \"Schweiz\", \"Schwäiz\"],\n    \"Tonga\": [\"Tonga\", \"Tonga\", \"Tonga\"],\n    \"Turks and Caicos Islands\": [\"Îles Turques-et-Caïques\", \"Turks- und Caicosinseln\", \"Turks- an Caicosinselen\"],\n    \"Vanuatu\": [\"Vanuatu\", \"Vanuatu\", \"Vanuatu\"],\n    \"Afghanistan\": [\"Afghanistan\", \"Afghanistan\", \"Afghanistan\"],\n    \"Albania\": [\"Albanie\", \"Albanien\", \"Albanien\"],\n    \"Algeria\": [\"Algérie\", \"Algerien\", \"Algerien\"],\n    \"Andorra\": [\"Andorre\", \"Andorra\", \"Andorra\"],\n    \"Angola\": [\"Angola\", \"Angola\", \"Angola\"],\n    \"Anguilla\": [\"Anguilla\", \"Anguilla\", \"Anguilla\"],\n    \"Antigua and Barbuda\": [\"Antigua-et-Barbuda\", \"Antigua und Barbuda\", \"Antigua an Barbuda\"],\n    \"Argentina\": [\"Argentine\", \"Argentinien\", \"Argentinien\"],\n    \"Armenia\": [\"Arménie\", \"Armenien\", \"Armenien\"],\n    \"Aruba\": [\"Aruba\", \"Aruba\", \"Aruba\"],\n    \"Australia\": [\"Australie\", \"Australien\", \"Australien\"],\n    \"Austria\": [\"Autriche\", \"Österreich\", \"Éisträich\"],\n    \"Azerbaijan\": [\"Azerbaïdjan\", \"Aserbaidschan\", \"Aserbaidschan\"],\n    \"Bahamas\": [\"Bahamas\", \"Bahamas\", \"Bahamas\"],\n    \"Bahrain\": [\"Bahreïn\", \"Bahrain\", \"Bahrain\"],\n    \"Bangladesh\": [\"Bangladesh\", \"Bangladesch\", \"Bangladesch\"],\n    \"Barbados\": [\"Barbade\", \"Barbados\", \"Barbados\"],\n    \"Belarus\": [\"Biélorussie\", \"Weißrussland\", \"Wäissrussland\"],\n    \"Belgium\": [\"Belgique\", \"Belgien\", \"Belsch\"],\n    \"Belize\": [\"Belize\", \"Belize\", \"Belize\"],\n    \"Benin\": [\"Bénin\", \"Benin\", \"Benin\"],\n    \"Bermuda\": [\"Bermudes\", \"Bermuda\", \"Bermuda\"],\n    \"Bhutan\": [\"Bhoutan\", \"Bhutan\", \"Bhutan\"],\n    \"Bolivia\": [\"Bolivie\", \"Bolivien\", \"Bolivien\"],\n    \"Bosnia and Herzegovina\": [\"Bosnie-Herzégovine\", \"Bosnien und Herzegowina\", \"Bosnien an Herzegowina\"],\n    \"Botswana\": [\"Botswana\", \"Botswana\", \"Botswana\"],\n    \"Brazil\": [\"Brésil\", \"Brasilien\", \"Brasilien\"],\n    \"British Virgin Islands\": [\"Îles Vierges britanniques\", \"Britische Jungferninseln\", \"Britesch Jofferinselen\"],\n    \"Brunei\": [\"Brunei\", \"Brunei\", \"Brunei\"],\n    \"Bulgaria\": [\"Bulgarie\", \"Bulgarien\", \"Bulgarien\"],\n    \"Burkina Faso\": [\"Burkina Faso\", \"Burkina Faso\", \"Burkina Faso\"],\n    \"Burundi\": [\"Burundi\", \"Burundi\", \"Burundi\"],\n    \"Cabo Verde\": [\"Cap-Vert\", \"Kap Verde\", \"Kap Verde\"],\n    \"Cambodia\": [\"Cambodge\", \"Kambodscha\", \"Kambodscha\"],\n    \"Cameroon\": [\"Cameroun\", \"Kamerun\", \"Kamerun\"],\n    \"Canada\": [\"Canada\", \"Kanada\", \"Kanada\"],\n    \"Cayman Islands\": [\"Îles Caïmans\", \"Kaimaninseln\", \"Kaimaninselen\"],\n    \"Central African Republic\": [\"République centrafricaine\", \"Zentralafrikanische Republik\", \"Zentralafrikanesch Republik\"],\n    \"Chad\": [\"Tchad\", \"Tschad\", \"Tschad\"],\n    \"Chile\": [\"Chili\", \"Chile\", \"Chile\"],\n    \"China\": [\"Chine\", \"China\", \"China\"],\n    \"Colombia\": [\"Colombie\", \"Kolumbien\", \"Kolumbien\"],\n    \"Comoros\": [\"Comores\", \"Komoren\", \"Komoren\"],\n    \"Congo\": [\"Congo\", \"Kongo\", \"Kongo\"],\n    \"Cook Islands\": [\"Îles Cook\", \"Cookinseln\", \"Cookinselen\"],\n    \"Costa Rica\": [\"Costa Rica\", \"Costa Rica\", \"Costa Rica\"],\n    \"Cote d'Ivoire\": [\"Côte d'Ivoire\", \"Elfenbeinküste\", \"Elfenbeinküst\"],\n    \"Croatia\": [\"Croatie\", \"Kroatien\", \"Kroatien\"],\n    \"Cuba\": [\"Cuba\", \"Kuba\", \"Kuba\"],\n    \"Curaçao\": [\"Curaçao\", \"Curaçao\", \"Curaçao\"],\n    \"Cyprus\": [\"Chypre\", \"Zypern\", \"Zypern\"],\n    \"Czech Republic\": [\"République tchèque\", \"Tschechien\", \"Tschechesch Republik\"],\n    \"Democratic Republic of the Congo\": [\"République démocratique du Congo\", \"Demokratische Republik Kongo\", \"Demokratesch Republik Kongo\"],\n    \"Denmark\": [\"Danemark\", \"Dänemark\", \"Dänemark\"],\n    \"Djibouti\": [\"Djibouti\", \"Dschibuti\", \"Dschibuti\"],\n    \"Dominican Republic\": [\"République dominicaine\", \"Dominikanische Republik\", \"Dominikanesch Republik\"],\n    \"Ecuador\": [\"Équateur\", \"Ecuador\", \"Ecuador\"],\n    \"Egypt\": [\"Égypte\", \"Ägypten\", \"Égypten\"],\n    \"El Salvador\": [\"Salvador\", \"El Salvador\", \"El Salvador\"],\n    \"Equatorial Guinea\": [\"Guinée équatoriale\", \"Äquatorialguinea\", \"Äquatorialguinea\"],\n    \"Eritrea\": [\"Érythrée\", \"Eritrea\", \"Eritrea\"],\n    \"Estonia\": [\"Estonie\", \"Estland\", \"Estland\"],\n    \"Ethiopia\": [\"Éthiopie\", \"Äthiopien\", \"Äthiopien\"],\n    \"Fiji\": [\"Fidji\", \"Fidschi\", \"Fidschi\"],\n    \"Finland\": [\"Finlande\", \"Finnland\", \"Finnland\"],\n    \"France\": [\"France\", \"Frankreich\", \"Frankräich\"],\n    \"Gabon\": [\"Gabon\", \"Gabun\", \"Gabun\"],\n    \"Gambia\": [\"Gambie\", \"Gambia\", \"Gambia\"],\n    \"Georgia\": [\"Géorgie\", \"Georgien\", \"Georgien\"],\n    \"Germany\": [\"Allemagne\", \"Deutschland\", \"Däitschland\"],\n    \"Ghana\": [\"Ghana\", \"Ghana\", \"Ghana\"],\n    \"Gibraltar\": [\"Gibraltar\", \"Gibraltar\", \"Gibraltar\"],\n    \"Greece\": [\"Grèce\", \"Griechenland\", \"Griicheland\"],\n    \"Grenada\": [\"Grenade\", \"Grenada\", \"Grenada\"],\n    \"Guatemala\": [\"Guatemala\", \"Guatemala\", \"Guatemala\"],\n    \"Guernsey\": [\"Guernesey\", \"Guernsey\", \"Guernsey\"],\n    \"Guinea\": [\"Guinée\", \"Guinea\", \"Guinea\"],\n    \"Guinea-Bissau\": [\"Guinée-Bissau\", \"Guinea-Bissau\", \"Guinea-Bissau\"],\n    \"Guyana\": [\"Guyana\", \"Guyana\", \"Guyana\"],\n    \"Haiti\": [\"Haïti\", \"Haiti\", \"Haiti\"],\n    \"Honduras\": [\"Honduras\", \"Honduras\", \"Honduras\"],\n    \"Hong Kong\": [\"Hong Kong\", \"Hongkong\", \"Hong Kong\"],\n    \"Hungary\": [\"Hongrie\", \"Ungarn\", \"Ungarn\"],\n    \"Iceland\": [\"Islande\", \"Island\", \"Island\"],\n    \"India\": [\"Inde\", \"Indien\", \"Indien\"],\n    \"Indonesia\": [\"Indonésie\", \"Indonesien\", \"Indonesien\"],\n    \"Iran\": [\"Iran\", \"Iran\", \"Iran\"],\n    \"Iraq\": [\"Irak\", \"Irak\", \"Irak\"],\n    \"Ireland\": [\"Irlande\", \"Irland\", \"Irland\"],\n    \"Isle of Man\": [\"Île de Man\", \"Isle of Man\", \"Isle of Man\"],\n    \"Israel\": [\"Israël\", \"Israel\", \"Israel\"],\n    \"Italy\": [\"Italie\", \"Italien\", \"Italien\"],\n    \"Jamaica\": [\"Jamaïque\", \"Jamaika\", \"Jamaika\"],\n    \"Japan\": [\"Japon\", \"Japan\", \"Japan\"],\n    \"Jersey\": [\"Jersey\", \"Jersey\", \"Jersey\"],\n    \"Jordan\": [\"Jordanie\", \"Jordanien\", \"Jordanien\"],\n    \"Kazakhstan\": [\"Kazakhstan\", \"Kasachstan\", \"Kasachstan\"],\n    \"Kenya\": [\"Kenya\", \"Kenia\", \"Kenia\"],\n    \"Kiribati\": [\"Kiribati\", \"Kiribati\", \"Kiribati\"],\n    \"Kuwait\": [\"Koweït\", \"Kuwait\", \"Kuwait\"],\n    \"Kyrgyzstan\": [\"Kirghizistan\", \"Kirgisistan\", \"Kirgisistan\"],\n    \"Laos\": [\"Laos\", \"Laos\", \"Laos\"],\n    \"Latvia\": [\"Lettonie\", \"Lettland\", \"Lettland\"],\n    \"Lebanon\": [\"Liban\", \"Libanon\", \"Libanon\"],\n    \"Lesotho\": [\"Lesotho\", \"Lesotho\", \"Lesotho\"],\n    \"Liberia\": [\"Libéria\", \"Liberia\", \"Liberia\"],\n    \"Libya\": [\"Libye\", \"Libyen\", \"Libyen\"],\n    \"Liechtenstein\": [\"Liechtenstein\", \"Liechtenstein\", \"Liechtenstein\"],\n    \"Lithuania\": [\"Lituanie\", \"Litauen\", \"Litauen\"],\n    \"Luxembourg\": [\"Luxembourg\", \"Luxemburg\", \"Lëtzebuerg\"],\n    \"Macao\": [\"Macao\", \"Macao\", \"Macao\"],\n    \"Madagascar\": [\"Madagascar\", \"Madagaskar\", \"Madagaskar\"],\n    \"Malawi\": [\"Malawi\", \"Malawi\", \"Malawi\"],\n    \"Malaysia\": [\"Malaisie\", \"Malaysia\", \"Malaysia\"],\n    \"Maldives\": [\"Maldives\", \"Malediven\", \"Malediven\"],\n    \"Mali\": [\"Mali\", \"Mali\", \"Mali\"],\n    \"Malta\": [\"Malte\", \"Malta\", \"Malta\"],\n    \"Marshall Islands\": [\"Îles Marshall\", \"Marshallinseln\", \"Marshallinselen\"],\n    \"Mauritania\": [\"Mauritanie\", \"Mauretanien\", \"Mauretanien\"],\n    \"Mauritius\": [\"Île Maurice\", \"Mauritius\", \"Mauritius\"],\n    \"Mexico\": [\"Mexique\", \"Mexiko\", \"Mexiko\"],\n    \"Micronesia\": [\"Micronésie\", \"Mikronesien\", \"Mikronesien\"],\n    \"Moldova\": [\"Moldavie\", \"Moldawien\", \"Moldawien\"],\n    \"Monaco\": [\"Monaco\", \"Monaco\", \"Monaco\"],\n    \"Mongolia\": [\"Mongolie\", \"Mongolei\", \"Mongolei\"],\n    \"Montenegro\": [\"Monténégro\", \"Montenegro\", \"Montenegro\"],\n    \"Montserrat\": [\"Montserrat\", \"Montserrat\", \"Montserrat\"],\n    \"Morocco\": [\"Maroc\", \"Marokko\", \"Marokko\"],\n    \"Mozambique\": [\"Mozambique\", \"Mosambik\", \"Mosambik\"],\n    \"Myanmar\": [\"Myanmar\", \"Myanmar\", \"Myanmar\"],\n    \"Namibia\": [\"Namibie\", \"Namibia\", \"Namibia\"],\n    \"Nauru\": [\"Nauru\", \"Nauru\", \"Nauru\"],\n    \"Nepal\": [\"Népal\", \"Nepal\", \"Nepal\"],\n    \"Netherlands\": [\"Pays-Bas\", \"Niederlande\", \"Holland\"],\n    \"New Zealand\": [\"Nouvelle-Zélande\", \"Neuseeland\", \"Neiséiland\"],\n    \"Nicaragua\": [\"Nicaragua\", \"Nicaragua\", \"Nicaragua\"],\n    \"Niger\": [\"Niger\", \"Niger\", \"Niger\"],\n    \"Nigeria\": [\"Nigeria\", \"Nigeria\", \"Nigeria\"],\n    \"Niue\": [\"Niue\", \"Niue\", \"Niue\"],\n    \"North Macedonia\": [\"Macédoine du Nord\", \"Nordmazedonien\", \"Nordmazedonien\"],\n    \"Norway\": [\"Norvège\", \"Norwegen\", \"Norwegen\"],\n    \"Oman\": [\"Oman\", \"Oman\", \"Oman\"],\n    \"Pakistan\": [\"Pakistan\", \"Pakistan\", \"Pakistan\"],\n    \"Palau\": [\"Palaos\", \"Palau\", \"Palau\"],\n    \"Palestine\": [\"Palestine\", \"Palästina\", \"Palästina\"],\n    \"Panama\": [\"Panama\", \"Panama\", \"Panama\"],\n    \"Papua New Guinea\": [\"Papouasie-Nouvelle-Guinée\", \"Papua-Neuguinea\", \"Papua-Neiguinea\"],\n    \"Paraguay\": [\"Paraguay\", \"Paraguay\", \"Paraguay\"],\n    \"Peru\": [\"Pérou\", \"Peru\", \"Peru\"],\n    \"Philippines\": [\"Philippines\", \"Philippinen\", \"Philippinnen\"],\n    \"Poland\": [\"Pologne\", \"Polen\", \"Polen\"],\n    \"Portugal\": [\"Portugal\", \"Portugal\", \"Portugal\"],\n    \"Qatar\": [\"Qatar\", \"Katar\", \"Katar\"],\n    \"Romania\": [\"Roumanie\", \"Rumänien\", \"Rumänien\"],\n    \"Russia\": [\"Russie\", \"Russland\", \"Russland\"],\n    \"Rwanda\": [\"Rwanda\", \"Ruanda\", \"Ruanda\"],\n    \"Saint Kitts and Nevis\": [\"Saint-Christophe-et-Niévès\", \"St. Kitts und Nevis\", \"St. Kitts an Nevis\"],\n    \"Saint Lucia\": [\"Sainte-Lucie\", \"St. Lucia\", \"St. Lucia\"],\n    \"Saint Martin\": [\"Saint-Martin\", \"Saint Martin\", \"Saint Martin\"],\n    \"Saint Vincent and the Grenadines\": [\"Saint-Vincent-et-les-Grenadines\", \"St. Vincent und die Grenadinen\", \"St. Vincent an d'Grenadinnen\"],\n    \"Samoa\": [\"Samoa\", \"Samoa\", \"Samoa\"],\n    \"San Marino\": [\"Saint-Marin\", \"San Marino\", \"San Marino\"],\n    \"Sao Tome and Principe\": [\"Sao Tomé-et-Principe\", \"São Tomé und Príncipe\", \"São Tomé an Príncipe\"],\n    \"Saudi Arabia\": [\"Arabie saoudite\", \"Saudi-Arabien\", \"Saudi-Arabien\"],\n    \"Senegal\": [\"Sénégal\", \"Senegal\", \"Senegal\"],\n    \"Serbia\": [\"Serbie\", \"Serbien\", \"Serbien\"],\n    \"Seychelles\": [\"Seychelles\", \"Seychellen\", \"Seychellen\"],\n    \"Sierra Leone\": [\"Sierra Leone\", \"Sierra Leone\", \"Sierra Leone\"],\n    \"Singapore\": [\"Singapour\", \"Singapur\", \"Singapur\"],\n    \"Slovakia\": [\"Slovaquie\", \"Slowakei\", \"Slowakei\"],\n    \"Slovenia\": [\"Slovénie\", \"Slowenien\", \"Slowenien\"],\n    \"Solomon Islands\": [\"Îles Salomon\", \"Salomonen\", \"Salomonen\"],\n    \"Somalia\": [\"Somalie\", \"Somalia\", \"Somalia\"],\n    \"South Africa\": [\"Afrique du Sud\", \"Südafrika\", \"Südafrika\"],\n    \"South Korea\": [\"Corée du Sud\", \"Südkorea\", \"Südkorea\"],\n    \"Spain\": [\"Espagne\", \"Spanien\", \"Spuenien\"],\n    \"Sri Lanka\": [\"Sri Lanka\", \"Sri Lanka\", \"Sri Lanka\"],\n    \"Sudan\": [\"Soudan\", \"Sudan\", \"Sudan\"],\n    \"Suriname\": [\"Suriname\", \"Suriname\", \"Suriname\"],\n    \"Sweden\": [\"Suède\", \"Schweden\", \"Schweden\"],\n    \"Switzerland\": [\"Suisse\", \"Schweiz\", \"Schwäiz\"],\n    \"Syria\": [\"Syrie\", \"Syrien\", \"Syrien\"],\n    \"Taiwan\": [\"Taïwan\", \"Taiwan\", \"Taiwan\"],\n    \"Tajikistan\": [\"Tadjikistan\", \"Tadschikistan\", \"Tadschikistan\"],\n    \"Tanzania\": [\"Tanzanie\", \"Tansania\", \"Tansania\"],\n    \"Thailand\": [\"Thaïlande\", \"Thailand\", \"Thailand\"],\n    \"Togo\": [\"Togo\", \"Togo\", \"Togo\"],\n    \"Tonga\": [\"Tonga\", \"Tonga\", \"Tonga\"],\n    \"Trinidad and Tobago\": [\"Trinité-et-Tobago\", \"Trinidad und Tobago\", \"Trinidad an Tobago\"],\n    \"Tunisia\": [\"Tunisie\", \"Tunesien\", \"Tunesien\"],\n    \"Turkey\": [\"Turquie\", \"Türkei\", \"Tierkei\"],\n    \"Turkmenistan\": [\"Turkménistan\", \"Turkmenistan\", \"Turkmenistan\"],\n    \"Turks and Caicos\": [\"Îles Turques-et-Caïques\", \"Turks- und Caicosinseln\", \"Turks- an Caicosinselen\"],\n    \"Tuvalu\": [\"Tuvalu\", \"Tuvalu\", \"Tuvalu\"],\n    \"Uganda\": [\"Ouganda\", \"Uganda\", \"Uganda\"],\n    \"Ukraine\": [\"Ukraine\", \"Ukraine\", \"Ukraine\"],\n    \"United Arab Emirates\": [\"Émirats arabes unis\", \"Vereinigte Arabische Emirate\", \"Vereenegt Arabesch Emirater\"],\n    \"United Kingdom\": [\"Royaume-Uni\", \"Vereinigtes Königreich\", \"Vereenegt Kinnekräich\"],\n    \"United States\": [\"États-Unis\", \"Vereinigte Staaten\", \"Vereenegt Staate\"],\n    \"Uruguay\": [\"Uruguay\", \"Uruguay\", \"Uruguay\"],\n    \"Uzbekistan\": [\"Ouzbékistan\", \"Usbekistan\", \"Usbekistan\"],\n    \"Vanuatu\": [\"Vanuatu\", \"Vanuatu\", \"Vanuatu\"],\n    \"Vatican City\": [\"Cité du Vatican\", \"Vatikanstadt\", \"Vatikanstad\"],\n    \"Venezuela\": [\"Venezuela\", \"Venezuela\", \"Venezuela\"],\n    \"Vietnam\": [\"Vietnam\", \"Vietnam\", \"Vietnam\"],\n    \"Yemen\": [\"Yémen\", \"Jemen\", \"Jemen\"],\n    \"Zambia\": [\"Zambie\", \"Sambia\", \"Sambia\"],\n    \"Zimbabwe\": [\"Zimbabwe\", \"Simbabwe\", \"Simbabwe\"]\n}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file=\"/JDH_PAPER/extracted_company_names_people_Addr.txt\"\nread=open(file,\"r\",encoding=\"utf-8\")\nwrite=open(\"/JDH_PAPER/final_a.txt\",\"w\",encoding=\"utf-8\")\nimport re\nfor line in read:\n    L=line.split(\"&&&***&&&\")\n    names_c=[]\n    for i in range(0, len(L)):\n        L[i] = re.sub(r'\\s+', ' ', L[i]).strip()\n    string_name_co=\" \"\n    if \"wrong\" in L[0].lower():\n        pass\n    else:\n        for each in countries.keys():\n            check=[each]\n            for e in countries[each]:\n                check.append(e)\n            # print(\"check: \",check)\n            for t_e in check:\n                # Regex to find `t_e` followed by a non-alphabetic character or end of string\n                pattern = rf\"\\b{re.escape(t_e)}\\b(?=\\s|[^a-zA-ZÀ-ÿ]|$)\"\n    \n                if re.search(pattern, L[-1]):\n                    if \"Jersey\"==t_e :\n                        aa=L[-1].count(\" Jersey\")\n                        bb=L[-1].count(\"New Jersey\")\n                        if bb< aa:\n                            names_c.append(each)\n                    else:\n                        names_c.append(each)\n            names_c=list(set(names_c))\n        for e in names_c:\n            string_name_co=string_name_co+e+\" *_* \"\n        string_name_co=re.sub(r'\\s+', ' ', string_name_co).strip()[:-4]\n    newline=\"\"\n    for i in range(0,len(L)-1):\n       newline=newline+L[i]+\" &&&***&&& \" \n    newline=newline+string_name_co+\" &&&***&&& \" +L[-1]\n    newline=re.sub(r'\\s+', ' ', newline).strip()\n    print(\"Company: \\n \",L[0])\n    print(\"Description: \\n \",L[-1].split(\"+++++++++++++++++\")[-1])\n    print(\"++++++++\")\n    print(\"Countries:\")\n    print(string_name_co)\n    print(\"++++++++\")\n\n    print(\"****************************************************************************************************\")\n\n    write.write(newline.replace(\"\\n\",\"\")+\"\\n\")\n    write.flush()\nwrite.close()\nprint(\"done\")","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Improvement Step: Removing Titles from Extracted People Names\n\nIn the address extraction pipeline, we identified the need to clean extracted people's names by removing unnecessary titles before proceeding with the loading phase. This additional step ensures cleaner and more consistent data by eliminating formal titles, job roles, and honorifics from the names. Below is a description of how this improvement works:\n\n\n### **Purpose**\nThe goal of this step is to:\n- Remove formal titles or honorifics (e.g., \"Dr.\", \"Prof.\", \"Monsieur\") from people's names.\n- Standardize the format of extracted names for improved accuracy during data analysis and downstream tasks.\n\n\n\n### **Implementation**\n\n1. **Removing Extra Spaces (`remove_extra_spaces`):**\n   - Uses a regular expression to replace multiple consecutive spaces with a single space.\n   - Strips leading and trailing spaces from the name to ensure a clean format.\n\n2. **Removing Titles (`remove_titles`):**\n   - **Title List:** Maintains a comprehensive list of titles across multiple languages:\n     - **English:** Titles like \"Mr\", \"Dr\", \"Prof\", \"Sir\".\n     - **French:** Titles like \"Monsieur\", \"Madame\", \"Président\".\n     - **German:** Titles like \"Herr\", \"Frau\", \"Doktor\".\n     - **Luxembourgish:** Titles like \"Här\", \"Madame\".\n     - **Spanish and Portuguese:** Titles like \"Señor\", \"Doutor\", \"Presidente\".\n   - **Regex Matching:** \n     - Creates a regex pattern to match any of the listed titles, including accented characters.\n     - Matches titles as standalone words (`\\b`) with optional trailing punctuation (e.g., \"Dr.\" or \"Dr\").\n     - Case-insensitive and Unicode-aware matching ensures flexibility.\n   - **Replacement:** Substitutes matched titles with an empty string, effectively removing them from the name.\n\n3. **Final Output:**\n   - Returns the cleaned name with no extra spaces or titles.\n\n\n\n### **Example Workflow**\n\n- **Input Name with Titles:**\n  - `\"Dr. John Doe\"`\n  - `\"Monsieur Jean Dupont\"`\n  - `\"Herr Karl Müller\"`\n\n- **Cleaned Name:**\n  - `\"John Doe\"`\n  - `\"Jean Dupont\"`\n  - `\"Karl Müller\"`\n\n\n### **Benefits**\n\n- **Improved Data Quality:**\n  - Ensures names are free from redundant titles, making them more uniform and easier to process.\n  \n- **Language-Agnostic Cleaning:**\n  - Handles titles across multiple languages, ensuring compatibility with multilingual datasets.\n  \n- **Flexibility:**\n  - The regex pattern can easily be extended to include additional titles as needed.\n\n\n### **Integration**\nThis cleaning step is applied before the load process in the pipeline, ensuring that only cleaned and title-free names are loaded into the final dataset.\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"def remove_extra_spaces(text):\n    # Use regular expression to replace multiple spaces with a single space\n    cleaned_text = re.sub(r'\\s+', ' ', text)\n    # Strip leading and trailing spaces\n    cleaned_text = cleaned_text.strip()\n    return cleaned_text\n    \ndef remove_titles(name):\n    import re\n\n    # List of common titles in English, French, German, Luxembourgish, Spanish, and Portuguese\n    titles = [\n        # English Titles\n        r'\\bMr\\b', r'\\bMrs\\b', r'\\bMs\\b', r'\\bMiss\\b', r'\\bDr\\b', r'\\bProf\\b', r'\\bSir\\b',\n        r'\\bMadam\\b', r'\\bLord\\b', r'\\bLady\\b', r'\\bRev\\b', r'\\bHon\\b', r'\\bJudge\\b',\n        r'\\bDame\\b', r'\\bCapt\\b', r'\\bCol\\b', r'\\bGen\\b', r'\\bLt\\b', r'\\bMaj\\b',\n        r'\\bSgt\\b', r'\\bCpl\\b', r'\\bPvt\\b', r'\\bChief\\b', r'\\bOfficer\\b', r'\\bDetective\\b',\n        r'\\bAttorney\\b', r'\\bAmb\\b', r'\\bConsul\\b', r'\\bSec\\b', r'\\bDir\\b', r'\\bMgr\\b',\n        r'\\bCEO\\b', r'\\bCFO\\b', r'\\bCTO\\b', r'\\bPresident\\b', r'\\bVP\\b', r'\\bDep\\b',\n\n        # French Titles\n        r'\\bMonsieur\\b', r'\\bMadame\\b', r'\\bMademoiselle\\b', r'\\bMlle\\b', r'\\bMme\\b', r'\\bMaître\\b',r'\\bMaitre\\b',r'\\bTissus\\b',\n        r'\\bDocteur\\b', r'\\bProfesseur\\b', r'\\bPrésident\\b', r'\\bVice-président\\b', r'\\bSecrétaire\\b',r'\\bLuxembourg\\b',r'\\bMaitre\\b',\n        r'\\bDirecteur\\b', r'\\bPDG\\b', r'\\bAdministrateur\\b', r'\\bComte\\b', r'\\bComtesse\\b', r'\\bComptes\\b',r'\\bBelgique\\b'\n        r'\\bMarquis\\b', r'\\bMarquise\\b', r'\\bDuc\\b', r'\\bDuchesse\\b', r'\\bPrince\\b', r'\\bPrincesse\\b',\n        r'\\bMïster\\b', r'\\bMessieurs\\b', # Mister with an ï (for completeness)\n\n        # German Titles\n        r'\\bHerr\\b', r'\\bFrau\\b', r'\\bFräulein\\b', r'\\bHr\\b', r'\\bFr\\b', r'\\bDoktor\\b',\n        r'\\bProfessor\\b', r'\\bPräsident\\b', r'\\bVizepräsident\\b', r'\\bSekretär\\b',\n        r'\\bDirektor\\b', r'\\bVerwalter\\b', r'\\bKanzler\\b', r'\\bBaron\\b', r'\\bBaronin\\b',\n        r'\\bGraf\\b', r'\\bGräfin\\b', r'\\bFürst\\b', r'\\bFürstin\\b', r'\\bHerzog\\b', r'\\bHerzogin\\b',\n\n        # Luxembourgish Titles\n        r'\\bHär\\b', r'\\bMadame\\b', r'\\bMademoiselle\\b', r'\\bDokter\\b', r'\\bProfesser\\b',\n        r'\\bBuergermeeschter\\b', r'\\bPrinz\\b', r'\\bPrinzessin\\b', r'\\bGroßherzog\\b', r'\\bGroßherzogin\\b',\n\n        # Spanish Titles\n        r'\\bSeñor\\b', r'\\bSeñora\\b', r'\\bSeñorita\\b', r'\\bDr\\b', r'\\bDra\\b', r'\\bProf\\b', r'\\bProfa\\b',\n        r'\\bDon\\b', r'\\bDoña\\b', r'\\bLicenciado\\b', r'\\bLicenciada\\b', r'\\bIngeniero\\b',\n        r'\\bIngeniera\\b', r'\\bPresidente\\b', r'\\bVicepresidente\\b', r'\\bDirector\\b',\n        r'\\bAdministradora\\b', r'\\bAlcalde\\b', r'\\bAlcaldesa\\b',\n\n        # Portuguese Titles\n        r'\\bSenhor\\b', r'\\bSenhora\\b', r'\\bSenhorita\\b', r'\\bDoutor\\b', r'\\bDoutora\\b',\n        r'\\bProfessor\\b', r'\\bProfessora\\b', r'\\bEngenheiro\\b', r'\\bEngenheira\\b',\n        r'\\bPresidente\\b', r'\\bVice-presidente\\b', r'\\bDiretor\\b', r'\\bDiretora\\b',\n        r'\\bAdministrador\\b', r'\\bAdministradora\\b', r'\\bPrefeito\\b', r'\\bPrefeita\\b'\n    ]\n\n    # Create a regex pattern that matches any of the titles (including accented characters)\n    pattern = re.compile(r'\\b(?:' + '|'.join(titles) + r')\\.?', re.IGNORECASE | re.UNICODE)\n\n    # Substitute the titles with an empty string\n    name_without_titles = re.sub(pattern, '', name).strip()\n    return name_without_titles\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nread=open(\"/JDH_PAPER/final_a.txt\",\"r\",encoding=\"utf-8\")\nwrite=open(\"/JDH_PAPER/final_a.txt\".replace(\"_a.txt\",\"_c.txt\"),\"w\",encoding=\"utf-8\")\n\ndelimiter=\"&&&***&&&\"\ncount=0\nfor line in read:\n    L=line.split(delimiter)\n    L[2]=remove_extra_spaces(L[2]).rstrip().lstrip().strip()\n    print(\"Input names: \",L[2])\n    print()\n    newL2=L[2].split(\" *_* \")\n\n    PEOPLE=[]\n    output_name=[]\n    for index in range(0,len(newL2)):\n        newL2[index]=remove_titles(remove_extra_spaces(newL2[index]).lstrip().rstrip().strip())\n        if len(newL2[index].split(\" \"))>1:\n            if len(newL2[index].split(\" \")[-1])>2:\n                output_name.append(remove_extra_spaces(newL2[index]).lstrip().rstrip().strip())\n    print(\"Cleaned names: \",output_name)\n    newline=\"\"\n    for i in range(0,len(L)-1):\n        if i!=2:\n            newline=newline+L[i]+\" \"+delimiter+\" \"\n\n        else:\n            output_string=\"\"\n            for each_n in output_name:\n                output_string=output_string+each_n+\" *_* \"\n            output_string=output_string[:-4].lstrip().rstrip().strip()\n            print(\"output_string:\",output_string)\n            newline=newline+output_string+\" \"+delimiter+\" \"\n    newline=newline+L[-1]\n    write.write(newline)\n    write.flush()\n    print()\n    print(\"*************************************************\")\nread.close()\nwrite.close()","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explanation of the Load Step in the ETL Process\n\nThe final step of the ETL (Extract, Transform, Load) process involves loading the processed and cleaned data into the relational database. This step ensures data integrity and establishes the necessary relationships between entities like companies, addresses, people, countries, and messages. Below is a detailed explanation of how the code accomplishes this:\n\n\n### **Key Components**\n\n1. **Database Connection:**\n   - **Connection String:** Establishes a connection to the SQL Server database using `pyodbc` with credentials (`UID`, `PWD`), server details, and the target database (`LETTERBOX`).\n   - **Error Handling:** Validates the connection and handles database connection errors gracefully.\n\n2. **Caching for Efficient Lookups:**\n   - **`fetch_all_keys`:** Preloads existing records from the database into Python dictionaries for efficient lookups.\n   - Avoids redundant queries to the database by caching entity IDs (e.g., `company_id`, `address_id`, etc.).\n\n3. **Helper Functions:**\n   - **`clean_value`:** Removes extra spaces and ensures values are consistently formatted before being loaded.\n   - **`insert_or_get_id`:** Handles:\n     - Checking if a record already exists in the database.\n     - Inserting a new record if it doesn't exist.\n     - Returning the primary key (ID) of the existing or newly inserted record.\n   - **`insert_into_junction`:** Inserts relationships into junction tables to establish many-to-many associations between entities.\n\n\n### **Loading Workflow**\n\n1. **Read and Parse Input File:**\n   - Reads the input file line by line.\n   - Splits each line into components (e.g., `company_name`, `address`, `people`, etc.) using the delimiter `&&&***&&&`.\n   - Cleans and validates each component.\n\n2. **Insert or Retrieve Entity Records:**\n   - **Companies:**\n     - Inserts or retrieves `company_id` for the provided company name.\n     - Handles alternate company names (`alt_company_name`) if the primary name contains \"wrong\".\n   - **Addresses:**\n     - Inserts or retrieves `address_id` for the cleaned address.\n     - Ensures address descriptions do not exceed a predefined maximum length.\n   - **People:**\n     - Inserts or retrieves `person_id` for each person in the list of names.\n   - **Countries:**\n     - Matches and retrieves country IDs from the database using the predefined `countries` dictionary.\n     - Establishes relationships between companies, addresses, and countries in the respective junction tables.\n\n3. **Insert Messages:**\n   - Inserts a `description` as a message into the `MessagesL` table.\n   - Associates messages with companies, addresses, and people via their respective junction tables.\n   - Uses `CAST` to ensure compatibility with `NVARCHAR` fields in the database.\n\n4. **Junction Tables:**\n   - Establishes many-to-many relationships between entities using junction tables:\n     - **`Companies_Countries`**: Links companies with their associated countries.\n     - **`Addresses_Countries`**: Links addresses with their respective countries.\n     - **`People_Companies`**: Links people with companies they are associated with.\n     - **`People_Addresses`**: Links people with the addresses they are connected to.\n     - **`People_MessagesL`, `Addresses_MessagesL`, and `Companies_MessagesL`**: Links people, addresses, and companies with the corresponding messages.\n\n5. **File Encoding Handling:**\n   - Determines the file encoding (`utf-8` or `latin-1`) based on file names, ensuring compatibility with legacy data.\n\n6. **Error Handling:**\n   - Captures and logs errors at every step, including:\n     - File reading issues.\n     - Database insertion or retrieval errors.\n     - Line processing errors, ensuring the process continues for subsequent lines.\n\n7. **Commit and Close:**\n   - Commits database transactions after successful insertions.\n   - Closes the database connection and cursor in the `finally` block to release resources.\n\n\n### **Benefits of the Load Step Implementation**\n\n- **Data Integrity:**\n  - Ensures no duplicate records are inserted into the database by checking for existing entries before inserting.\n  - Maintains consistent and clean data formatting through `clean_value`.\n\n- **Efficient Relationships:**\n  - Establishes robust many-to-many relationships between entities, enabling complex queries and analyses.\n\n- **Scalability:**\n  - Uses caching and batch processing to handle large datasets efficiently.\n\n- **Error Resilience:**\n  - Implements extensive error handling to ensure the process continues despite individual record errors.\n\n\n### **Summary**\nThis load step finalizes the ETL pipeline by integrating processed data into a relational database while maintaining data integrity, establishing relationships, and preparing the data for downstream applications or analysis.\n\n\n\n\n\n\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"markdown","source":"## Explanation of the Load Step in the ETL Process\n\nThe final step of the ETL (Extract, Transform, Load) process involves loading the processed and cleaned data into the relational database. This step ensures data integrity and establishes the necessary relationships between entities like companies, addresses, people, countries, and messages. Below is a detailed explanation of how the code accomplishes this:\n\n\n### **Key Components**\n\n1. **Database Connection:**\n   - **Connection String:** Establishes a connection to the SQL Server database using `pyodbc` with credentials (`UID`, `PWD`), server details, and the target database (`LETTERBOX`).\n   - **Error Handling:** Validates the connection and handles database connection errors gracefully.\n\n2. **Caching for Efficient Lookups:**\n   - **`fetch_all_keys`:** Preloads existing records from the database into Python dictionaries for efficient lookups.\n   - Avoids redundant queries to the database by caching entity IDs (e.g., `company_id`, `address_id`, etc.).\n\n3. **Helper Functions:**\n   - **`clean_value`:** Removes extra spaces and ensures values are consistently formatted before being loaded.\n   - **`insert_or_get_id`:** Handles:\n     - Checking if a record already exists in the database.\n     - Inserting a new record if it doesn't exist.\n     - Returning the primary key (ID) of the existing or newly inserted record.\n   - **`insert_into_junction`:** Inserts relationships into junction tables to establish many-to-many associations between entities.\n\n\n### **Loading Workflow**\n\n1. **Read and Parse Input File:**\n   - Reads the input file line by line.\n   - Splits each line into components (e.g., `company_name`, `address`, `people`, etc.) using the delimiter `&&&***&&&`.\n   - Cleans and validates each component.\n\n2. **Insert or Retrieve Entity Records:**\n   - **Companies:**\n     - Inserts or retrieves `company_id` for the provided company name.\n     - Handles alternate company names (`alt_company_name`) if the primary name contains \"wrong\".\n   - **Addresses:**\n     - Inserts or retrieves `address_id` for the cleaned address.\n     - Ensures address descriptions do not exceed a predefined maximum length.\n   - **People:**\n     - Inserts or retrieves `person_id` for each person in the list of names.\n   - **Countries:**\n     - Matches and retrieves country IDs from the database using the predefined `countries` dictionary.\n     - Establishes relationships between companies, addresses, and countries in the respective junction tables.\n\n3. **Insert Messages:**\n   - Inserts a `description` as a message into the `MessagesL` table.\n   - Associates messages with companies, addresses, and people via their respective junction tables.\n   - Uses `CAST` to ensure compatibility with `NVARCHAR` fields in the database.\n\n4. **Junction Tables:**\n   - Establishes many-to-many relationships between entities using junction tables:\n     - **`Companies_Countries`**: Links companies with their associated countries.\n     - **`Addresses_Countries`**: Links addresses with their respective countries.\n     - **`People_Companies`**: Links people with companies they are associated with.\n     - **`People_Addresses`**: Links people with the addresses they are connected to.\n     - **`People_MessagesL`, `Addresses_MessagesL`, and `Companies_MessagesL`**: Links people, addresses, and companies with the corresponding messages.\n\n5. **File Encoding Handling:**\n   - Determines the file encoding (`utf-8` or `latin-1`) based on file names, ensuring compatibility with legacy data.\n\n6. **Error Handling:**\n   - Captures and logs errors at every step, including:\n     - File reading issues.\n     - Database insertion or retrieval errors.\n     - Line processing errors, ensuring the process continues for subsequent lines.\n\n7. **Commit and Close:**\n   - Commits database transactions after successful insertions.\n   - Closes the database connection and cursor in the `finally` block to release resources.\n\n\n### **Benefits of the Load Step Implementation**\n\n- **Data Integrity:**\n  - Ensures no duplicate records are inserted into the database by checking for existing entries before inserting.\n  - Maintains consistent and clean data formatting through `clean_value`.\n\n- **Efficient Relationships:**\n  - Establishes robust many-to-many relationships between entities, enabling complex queries and analyses.\n\n- **Scalability:**\n  - Uses caching and batch processing to handle large datasets efficiently.\n\n- **Error Resilience:**\n  - Implements extensive error handling to ensure the process continues despite individual record errors.\n\n\n### **Summary**\nThis load step finalizes the ETL pipeline by integrating processed data into a relational database while maintaining data integrity, establishing relationships, and preparing the data for downstream applications or analysis.\n\n","metadata":{"tags":["disclaimer"]}},{"cell_type":"code","source":"import pyodbc\n\n# Define your connection string with the correct credentials and server information\nconn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.184.4.29;DATABASE=LETTER;UID=c2dh;PWD=C2dh4ever'\n\n# Establish the connection\ntry:\n    conn = pyodbc.connect(conn_str)\n    cursor = conn.cursor()\n    print(\"Connection successful\")\nexcept pyodbc.Error as e:\n    print(f\"Error connecting to the database: {e}\")\n    raise\n\n# List of tables to delete from\n# Note: Order matters - delete from the junction tables first to avoid foreign key constraint violations\ntables = [\n    'People_Addresses',\n    'People_Companies',\n    'People_Countries',\n    'People_MessagesL',\n    'Addresses_Countries',\n    'Companies_Countries',\n    'People',\n    'Addresses',\n    'Companies',\n    'Countries',\n    'MessagesL',\n]\n\n# Function to delete all rows from the tables\ndef delete_all_data():\n    try:\n        for table in tables:\n            print(f\"Deleting data from {table}...\")\n            cursor.execute(f\"DELETE FROM {table}\")  # Using DELETE to respect foreign key constraints\n            conn.commit()\n            print(f\"Data deleted from {table}\")\n    except pyodbc.Error as e:\n        print(f\"Error deleting data from {table}: {e}\")\n        raise\n\n# Run the delete process\ndelete_all_data()\n\n# Close the connection\ncursor.close()\nconn.close()\nprint(\"Connection closed\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyodbc\nimport re\nimport traceback\n\n\ndef has_column(table, column):\n    \"\"\"\n    Check if a column exists in a specific table.\n    \"\"\"\n    try:\n        cursor.execute(\"\"\"\n            SELECT 1\n            FROM INFORMATION_SCHEMA.COLUMNS\n            WHERE TABLE_NAME = ? AND COLUMN_NAME = ?\n        \"\"\", table, column)\n        return cursor.fetchone() is not None\n    except pyodbc.Error as e:\n        print(f\"Error checking column {column} in table {table}: {e}\")\n        return False\n\n\ndef assign_unused_id(table, id_column):\n    \"\"\"\n    Retrieve the next unused ID for a given table and column.\n    \"\"\"\n    try:\n        cursor.execute(f\"SELECT MAX({id_column}) FROM {table}\")\n        max_id = cursor.fetchone()[0]\n        return (max_id or 0) + 1\n    except pyodbc.Error as e:\n        print(f\"Error fetching max ID from {table}: {e}\")\n        raise\n\n\ndef insert_or_get_id(table, column, value, year=None, id_column=None):\n    \"\"\"\n    Insert a new record or retrieve the ID of an existing record.\n    \"\"\"\n\n    if not id_column:\n        id_column = f\"{table[:-1]}_id\"\n\n    try:\n        # Check if the record already exists\n        query = f\"SELECT {id_column} FROM {table} WHERE {column} = ?\"\n        params = [value]\n        if has_column(table, \"year\"):\n            query += \" AND year = ?\"\n            params.append(year)\n\n        cursor.execute(query, params)\n        row = cursor.fetchone()\n\n        # Return the ID if the record exists\n        if row:\n            return row[0]\n\n        # Assign new ID\n        new_id = assign_unused_id(table, id_column)\n\n        # Insert new record\n        columns = [id_column, column]\n        values = [new_id, value]\n        placeholders = [\"?\", \"?\"]\n\n        if has_column(table, \"year\") and year is not None:\n            columns.append(\"year\")\n            values.append(year)\n            placeholders.append(\"?\")\n\n        if has_column(table, \"effective_start_date\"):\n            columns.append(\"effective_start_date\")\n            placeholders.append(\"?\")\n            values.append(\"1970-01-01\")  # Default start date\n\n        if has_column(table, \"is_current\"):\n            columns.append(\"is_current\")\n            placeholders.append(\"?\")\n            values.append(1)  # Default value for is_current\n\n        if has_column(table, \"current_version\"):\n            columns.append(\"current_version\")\n            placeholders.append(\"?\")\n            values.append(1)  # Default version for new records\n\n        insert_query = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(placeholders)})\"\n        cursor.execute(insert_query, values)\n        conn.commit()\n\n        return new_id\n\n    except pyodbc.Error as e:\n        print(f\"Error inserting or retrieving ID from {table} for value '{value}': {e}\")\n        return None\n\n\ndef insert_into_junction(table, col1, col2, year, id1, id2):\n    \"\"\"\n    Insert or update a relationship in a junction table using SCD Type 2 logic.\n    \"\"\"\n    try:\n        # Check if an active record exists\n        cursor.execute(f\"\"\"\n            SELECT is_current, effective_start_date, effective_end_date\n            FROM {table}\n            WHERE {col1} = ? AND {col2} = ? AND year = ?\n        \"\"\", id1, id2, year)\n        row = cursor.fetchone()\n\n        if row and row[0] == 1:  # Active record exists\n            return  # No need to update\n\n        if row:  # Inactivate the existing record\n            cursor.execute(f\"\"\"\n                UPDATE {table}\n                SET is_current = 0, effective_end_date = GETDATE()\n                WHERE {col1} = ? AND {col2} = ? AND year = ? AND is_current = 1\n            \"\"\", id1, id2, year)\n            conn.commit()\n\n        # Insert the new record\n        cursor.execute(f\"\"\"\n            INSERT INTO {table} ({col1}, {col2}, year, is_current, effective_start_date, effective_end_date)\n            VALUES (?, ?, ?, 1, GETDATE(), NULL)\n        \"\"\", id1, id2, year)\n        conn.commit()\n    except pyodbc.Error as e:\n        print(f\"Error inserting into {table} (col1={col1}, col2={col2}, year={year}): {e}\")\n        raise\n\n\n\ndef process_file(path_file):\n    \"\"\"\n    Process the input file and populate the database with its data.\n    \"\"\"\n    try:\n        line_number=0\n        with open(path_file, 'r', encoding='utf-8') as file:\n            for line in file:\n                # Split the line into columns\n                columns = line.strip().split(' &&&***&&& ')\n                # Extract the data from the line\n                year = int(columns[0].strip())\n                company_name = columns[1]\n                address = columns[3]\n                \n                country_name = columns[7].split(\"*_*\")\n                for index_country in range(0,len(country_name)):\n                    country_name[index_country]=country_name[index_country].rstrip().lstrip().strip()\n                \n                person_name = columns[5].split(\"*_*\")\n                for person_index in range(0,len(person_name)):\n                    person_name[person_index]=person_name[person_index].rstrip().lstrip().strip()\n\n                try:\n                    person_name.remove('')\n                except:\n                    pass\n\n                try:\n                    person_name.remove('')\n                except:\n                    pass\n                \n                if \"wrong\" in company_name.lower():\n                    continue\n                else:\n                # Insert or retrieve the IDs\n                    country_ids=[]\n                    person_ids=[]\n                    \n                    company_id = insert_or_get_id('Companies', 'company_name', company_name, year, 'company_id')\n                    address_id = insert_or_get_id('Addresses', 'address_description', address, year, 'address_id')\n\n                    for country in country_name:\n                        country_id = insert_or_get_id('Countries', 'country_name', country, year, 'country_id')\n                        country_ids.append(country_id)\n                            \n                    for person in person_name:\n                        person_id = insert_or_get_id('People', 'person_name', person, year, 'person_id')\n                        person_ids.append(person_id)\n                            \n                    # Insert into junction tables\n                    for country_id in country_ids:\n                        if company_id and country_id:\n                            print(\"company_id:\",company_id)\n                            print(\"country_id:\",country_id)\n                            insert_into_junction('Companies_Countries', 'company_id', 'country_id', year, company_id, country_id)\n                        if address_id and country_id:\n                            insert_into_junction('Addresses_Countries', 'address_id', 'country_id', year, address_id, country_id)\n\n                    for person_id in person_ids:\n                        if person_id and company_id:\n                            insert_into_junction('People_Companies', 'person_id', 'company_id', year, person_id, company_id)\n                        if person_id and address_id:\n                            insert_into_junction('People_Addresses', 'person_id', 'address_id', year, person_id, address_id)\n\n                    \n                    print(\"line_number:\",line_number)\n                line_number=line_number+1\n\n    except FileNotFoundError:\n        print(f\"File not found: {path_file}\")\n    except pyodbc.Error as e:\n        print(f\"Database error while processing file: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n\nif __name__ == \"__main__\":\n    conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.184.4.29;DATABASE=LETTER;UID=c2dh;PWD=C2dh4ever'\n\n    for year in range(1970, 1971):\n        try:\n            conn = pyodbc.connect(conn_str)\n            cursor = conn.cursor()\n            print(f\"Processing data for year {year}...\")\n            path_file = f\"/home/mehrdad/FILE_IMPORT/{year}_final_2ADDR_final_check_c.txt\"\n            process_file(path_file)\n        except Exception as e:\n            print(f\"Error: {e}\")\n        finally:\n            try:\n                cursor.close()\n                conn.close()\n            except Exception:\n                pass\n","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# python3 CoWorker_Names\n# CHECK SCD type2\nSELECT *\nFROM People\nWHERE person_id IN (25, 40);\n\n\n-- Mark the current record as inactive\nUPDATE People\nSET is_current = 0,\n    effective_end_date = GETDATE()\nWHERE person_id = 25;\n\n--Update\n-- Insert the new version of the record\nINSERT INTO People (person_id, person_name, year, current_version, effective_start_date, is_current)\nVALUES (400048, 'HANS STAHL', 1970, 2, GETDATE(), 1);\n\n--3. Display Updated State\n-- Query to display the data after the update to verify the changes:\nSELECT *\nFROM People\nWHERE person_id IN (25, 40, 400048);\n\n\n--4. Revert Step\n--Simulate reverting back to the original state:\n\n--Mark the new \"HANS STAHL\" record (ID: 48) as inactive and set its effective_end_date.\n--Reactivate the original \"ERNST MOMMSEN\" record by setting is_current = 1 and clearing effective_end_date\n\n-- Mark the new version as inactive\nUPDATE People\nSET is_current = 0,\n    effective_end_date = GETDATE()\nWHERE person_id = 400048;\n\n-- Reactivate the original record\nUPDATE People\nSET is_current = 1,\n    effective_end_date = NULL\nWHERE person_id = 25;\n\n\n-- 5. Display Final State\n-- Query to confirm that the original state has been restored:\n\nSELECT *\nFROM People\nWHERE person_id IN (25, 40, 400048);\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}